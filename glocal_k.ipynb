{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrainSack/recommendation-system/blob/main/glocal_k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty3gYQgtnwFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845ec622-64eb-4c87-f997-2c1b7b17b376"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9L3y-n0wU5F",
        "outputId": "3753d40a-3e48-4741-b5e4-4a849543ceb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.9.2\n",
            "Uninstalling tensorflow-2.9.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.9.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McNeEqRswb1k",
        "outputId": "9c039fb8-2887-491a-b326-2510a0475f4b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "y\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 27 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.50.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.21.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.38.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (2.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 53.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.1.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7555 sha256=78a3badbdc5fde1b33374269c4e6648267ce0e047593b1888c7880ff5d8db317\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.17.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "source": [
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcyWCv_WvSKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570d22a6-e472-426b-9651-978cee1d02c8"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "ki6rqtDmqyhd",
        "outputId": "e0a6779b-f79e-4020-cc29-a65c512628de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "source": [
        "def load_data_100k(path='./content/drive/MyDrive/MovieLens_100K', delimiter='\\t'):\n",
        "\n",
        "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    total = np.concatenate((train, test), axis=0)\n",
        "\n",
        "    n_u = np.unique(total[:,0]).size  # num of users\n",
        "    n_m = np.unique(total[:,1]).size  # num of movies\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_train):\n",
        "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3e8Xg3us8g7"
      },
      "source": [
        "def load_data_1m(path='./', delimiter='::', frac=0.1, seed=1234):\n",
        "\n",
        "    tic = time()\n",
        "    print('reading data...')\n",
        "    data = np.loadtxt(path+'movielens_1m_dataset.dat', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    print('taken', time() - tic, 'seconds')\n",
        "\n",
        "    n_u = np.unique(data[:,0]).size  # num of users\n",
        "    n_m = np.unique(data[:,1]).size  # num of movies\n",
        "    n_r = data.shape[0]  # num of ratings\n",
        "\n",
        "    udict = {}\n",
        "    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n",
        "        udict[u] = i\n",
        "    mdict = {}\n",
        "    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n",
        "        mdict[m] = i\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    idx = np.arange(n_r)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_r):\n",
        "        u_id = data[idx[i], 0]\n",
        "        m_id = data[idx[i], 1]\n",
        "        r = data[idx[i], 2]\n",
        "\n",
        "        if i < int(frac * n_r):\n",
        "            test_r[mdict[m_id], udict[u_id]] = r\n",
        "        else:\n",
        "            train_r[mdict[m_id], udict[u_id]] = r\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n",
        "    print('num of test ratings: {}'.format(int(frac * n_r)))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rMjcbLvhtRs"
      },
      "source": [
        "def load_matlab_file(path_file, name_field):\n",
        "    \n",
        "    db = h5py.File(path_file, 'r')\n",
        "    ds = db[name_field]\n",
        "\n",
        "    try:\n",
        "        if 'ir' in ds.keys():\n",
        "            data = np.asarray(ds['data'])\n",
        "            ir   = np.asarray(ds['ir'])\n",
        "            jc   = np.asarray(ds['jc'])\n",
        "            out  = csc_matrix((data, ir, jc)).astype(np.float32)\n",
        "    except AttributeError:\n",
        "        out = np.asarray(ds).astype(np.float32).T\n",
        "\n",
        "    db.close()\n",
        "\n",
        "    return out"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6pIUrkza2zv"
      },
      "source": [
        "def load_data_monti(path='./'):\n",
        "\n",
        "    M = load_matlab_file(path+'douban_monti_dataset.mat', 'M')\n",
        "    Otraining = load_matlab_file(path+'douban_monti_dataset.mat', 'Otraining') * M\n",
        "    Otest = load_matlab_file(path+'douban_monti_dataset.mat', 'Otest') * M\n",
        "\n",
        "    n_u = M.shape[0]  # num of users\n",
        "    n_m = M.shape[1]  # num of movies\n",
        "    n_train = Otraining[np.where(Otraining)].size  # num of training ratings\n",
        "    n_test = Otest[np.where(Otest)].size  # num of test ratings\n",
        "\n",
        "    train_r = Otraining.T\n",
        "    test_r = Otest.T\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fkA1WpmipzF"
      },
      "source": [
        "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "data_path = '/content/drive/'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijlu0lXQioYM"
      },
      "source": [
        "# Select a dataset among 'ML-1M', 'ML-100K', and 'Douban'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "dataset = 'ML-100K'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqSSY33mgkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e8ba75-0e28-4d03-f2d4-c7b3e23ee068"
      },
      "source": [
        "# Data Load\n",
        "try:\n",
        "    if dataset == 'ML-100K':\n",
        "        path = '/content/drive/MyDrive/MovieLens_100K/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')\n",
        "\n",
        "    elif dataset == 'ML-1M':\n",
        "        path = data_path + 'MyDrive/MovieLens_1M/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_1m(path=path, delimiter='::', frac=0.1, seed=1234)\n",
        "\n",
        "    elif dataset == 'Douban':\n",
        "        path = data_path + 'MyDrive/Douban_monti/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_monti(path=path)\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "except ValueError:\n",
        "    print('Error: Unable to load data')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data matrix loaded\n",
            "num of users: 943\n",
            "num of movies: 1682\n",
            "num of training ratings: 80000\n",
            "num of test ratings: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQMtA9yml-gp"
      },
      "source": [
        "# Hyperparameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCdp_FlobOK"
      },
      "source": [
        "# Common hyperparameter settings\n",
        "n_hid = 500\n",
        "n_dim = 5\n",
        "n_layers = 2\n",
        "gk_size = 3"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "344bwGB0cWXp"
      },
      "source": [
        "# Different hyperparameter settings for each dataset\n",
        "if dataset == 'ML-100K':\n",
        "    lambda_2 = 20.  # l2 regularisation\n",
        "    lambda_s = 0.006\n",
        "    iter_p = 5  # optimisation\n",
        "    iter_f = 5\n",
        "    epoch_p = 30  # training epoch\n",
        "    epoch_f = 60\n",
        "    dot_scale = 1  # scaled dot product\n",
        "\n",
        "elif dataset == 'ML-1M':\n",
        "    lambda_2 = 70.\n",
        "    lambda_s = 0.018\n",
        "    iter_p = 50\n",
        "    iter_f = 10\n",
        "    epoch_p = 20\n",
        "    epoch_f = 30\n",
        "    dot_scale = 0.5\n",
        "\n",
        "elif dataset == 'Douban':\n",
        "    lambda_2 = 10.\n",
        "    lambda_s = 0.022\n",
        "    iter_p = 5\n",
        "    iter_f = 5\n",
        "    epoch_p = 20\n",
        "    epoch_f = 60\n",
        "    dot_scale = 2"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94aimX3nAMI"
      },
      "source": [
        "R = tf.placeholder(\"float\", [n_m, n_u])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX2wREO09zde"
      },
      "source": [
        "def local_kernel(u, v):\n",
        "\n",
        "    dist = tf.norm(u - v, ord=2, axis=2)\n",
        "    hat = tf.maximum(0., 1. - dist**2)\n",
        "\n",
        "    return hat"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c88l9LYr9175"
      },
      "source": [
        "def kernel_layer(x, n_hid=n_hid, n_dim=n_dim, activation=tf.nn.sigmoid, lambda_s=lambda_s, lambda_2=lambda_2, name=''):\n",
        "\n",
        "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
        "        n_in = x.get_shape().as_list()[1]\n",
        "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
        "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
        "        b = tf.get_variable('b', [n_hid])\n",
        "\n",
        "    w_hat = local_kernel(u, v)\n",
        "    \n",
        "    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
        "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
        "    \n",
        "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
        "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
        "\n",
        "    W_eff = W * w_hat  # Local kernelised weight matrix\n",
        "    y = tf.matmul(x, W_eff) + b\n",
        "    y = activation(y)\n",
        "\n",
        "    return y, sparse_reg_term + l2_reg_term"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlb95FmRVATa"
      },
      "source": [
        "def global_kernel(input, gk_size, dot_scale):\n",
        "\n",
        "    avg_pooling = tf.reduce_mean(input, axis=1)  # Item (axis=1) based average pooling\n",
        "    avg_pooling = tf.reshape(avg_pooling, [1, -1])\n",
        "    n_kernel = avg_pooling.shape[1].value\n",
        "\n",
        "    conv_kernel = tf.get_variable('conv_kernel', initializer=tf.random.truncated_normal([n_kernel, gk_size**2], stddev=0.1))\n",
        "    gk = tf.matmul(avg_pooling, conv_kernel) * dot_scale  # Scaled dot product\n",
        "    gk = tf.reshape(gk, [gk_size, gk_size, 1, 1])\n",
        "\n",
        "    return gk"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTLi_65XzIbH"
      },
      "source": [
        "def global_conv(input, W):\n",
        "\n",
        "    input = tf.reshape(input, [1, input.shape[0], input.shape[1], 1])\n",
        "    conv2d = tf.nn.relu(tf.nn.conv2d(input, W, strides=[1,1,1,1], padding='SAME'))\n",
        "\n",
        "    return tf.reshape(conv2d, [conv2d.shape[1], conv2d.shape[2]])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "## Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7teUrgWagpW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6b29bb-3c1a-453c-d74c-0abc380d25b3"
      },
      "source": [
        "y = R\n",
        "reg_losses = None\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y, reg_loss = kernel_layer(y, name=str(i))\n",
        "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "pred_p, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
        "reg_losses = reg_losses + reg_loss\n",
        "\n",
        "# L2 loss\n",
        "diff = train_m * (train_r - pred_p)\n",
        "sqE = tf.nn.l2_loss(diff)\n",
        "loss_p = sqE + reg_losses\n",
        "\n",
        "optimizer_p = tf.contrib.opt.ScipyOptimizerInterface(loss_p, options={'disp': True, 'maxiter': iter_p, 'maxcor': 10}, method='L-BFGS-B')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiTXqnN6zLXQ"
      },
      "source": [
        "y = R\n",
        "reg_losses = None\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y, _ = kernel_layer(y, name=str(i))\n",
        "\n",
        "y_dash, _ = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
        "\n",
        "gk = global_kernel(y_dash, gk_size, dot_scale)  # Global kernel\n",
        "y_hat = global_conv(train_r, gk)  # Global kernel-based rating matrix\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y_hat, reg_loss = kernel_layer(y_hat, name=str(i))\n",
        "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "pred_f, reg_loss = kernel_layer(y_hat, n_u, activation=tf.identity, name='out')\n",
        "reg_losses = reg_losses + reg_loss\n",
        "\n",
        "# L2 loss\n",
        "diff = train_m * (train_r - pred_f)\n",
        "sqE = tf.nn.l2_loss(diff)\n",
        "loss_f = sqE + reg_losses\n",
        "\n",
        "optimizer_f = tf.contrib.opt.ScipyOptimizerInterface(loss_f, options={'disp': True, 'maxiter': iter_f, 'maxcor': 10}, method='L-BFGS-B')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation code"
      ],
      "metadata": {
        "id": "sETwz58aK6y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ],
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ],
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ],
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Training and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ35Zoha-Eue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27d662cf-a884-4caa-ff72-6ae461160d04"
      },
      "source": [
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "\n",
        "time_cumulative = 0\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in range(epoch_p):\n",
        "        tic = time()\n",
        "        optimizer_p.minimize(sess, feed_dict={R: train_r})\n",
        "        pre = sess.run(pred_p, feed_dict={R: train_r})\n",
        "\n",
        "        t = time() - tic\n",
        "        time_cumulative += t\n",
        "        \n",
        "        error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "        test_rmse = np.sqrt(error)\n",
        "\n",
        "        error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "        train_rmse = np.sqrt(error_train)\n",
        "\n",
        "        print('.-^-._' * 12)\n",
        "        print('PRE-TRAINING')\n",
        "        print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "        print('Time:', t, 'seconds')\n",
        "        print('Time cumulative:', time_cumulative, 'seconds')\n",
        "        print('.-^-._' * 12)\n",
        "\n",
        "    for i in range(epoch_f):\n",
        "        tic = time()\n",
        "        optimizer_f.minimize(sess, feed_dict={R: train_r})\n",
        "        pre = sess.run(pred_f, feed_dict={R: train_r})\n",
        "\n",
        "        t = time() - tic\n",
        "        time_cumulative += t\n",
        "        \n",
        "        error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "        test_rmse = np.sqrt(error)\n",
        "\n",
        "        error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "        train_rmse = np.sqrt(error_train)\n",
        "\n",
        "        test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
        "        train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
        "\n",
        "        test_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
        "        train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "        if test_rmse < best_rmse:\n",
        "            best_rmse = test_rmse\n",
        "            best_rmse_ep = i+1\n",
        "\n",
        "        if test_mae < best_mae:\n",
        "            best_mae = test_mae\n",
        "            best_mae_ep = i+1\n",
        "\n",
        "        if best_ndcg < test_ndcg:\n",
        "            best_ndcg = test_ndcg\n",
        "            best_ndcg_ep = i+1\n",
        "\n",
        "        print('.-^-._' * 12)\n",
        "        print('FINE-TUNING')\n",
        "        print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "        print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "        print('Time:', t, 'seconds')\n",
        "        print('Time cumulative:', time_cumulative, 'seconds')\n",
        "        print('.-^-._' * 12)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 1 test rmse: 1.4202852 train rmse: 1.258528\n",
            "Time: 7.526934623718262 seconds\n",
            "Time cumulative: 7.526934623718262 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 2 test rmse: 1.0740693 train rmse: 1.0121998\n",
            "Time: 4.241316795349121 seconds\n",
            "Time cumulative: 11.768251419067383 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 3 test rmse: 1.0325727 train rmse: 0.9868746\n",
            "Time: 4.284182786941528 seconds\n",
            "Time cumulative: 16.05243420600891 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 4 test rmse: 1.013358 train rmse: 0.9718383\n",
            "Time: 4.5147998332977295 seconds\n",
            "Time cumulative: 20.56723403930664 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 5 test rmse: 0.99492943 train rmse: 0.9553933\n",
            "Time: 4.86611270904541 seconds\n",
            "Time cumulative: 25.43334674835205 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 6 test rmse: 0.9839389 train rmse: 0.94551396\n",
            "Time: 4.4431517124176025 seconds\n",
            "Time cumulative: 29.876498460769653 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 7 test rmse: 0.9759014 train rmse: 0.93718314\n",
            "Time: 4.863722324371338 seconds\n",
            "Time cumulative: 34.74022078514099 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 8 test rmse: 0.9694036 train rmse: 0.9307529\n",
            "Time: 4.6511523723602295 seconds\n",
            "Time cumulative: 39.39137315750122 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 9 test rmse: 0.9679753 train rmse: 0.9270716\n",
            "Time: 4.985898971557617 seconds\n",
            "Time cumulative: 44.37727212905884 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 10 test rmse: 0.9615606 train rmse: 0.92049104\n",
            "Time: 6.205792665481567 seconds\n",
            "Time cumulative: 50.583064794540405 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 11 test rmse: 0.95401436 train rmse: 0.90611166\n",
            "Time: 5.152094841003418 seconds\n",
            "Time cumulative: 55.73515963554382 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 12 test rmse: 0.94653976 train rmse: 0.8989325\n",
            "Time: 6.288124322891235 seconds\n",
            "Time cumulative: 62.02328395843506 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 13 test rmse: 0.9447656 train rmse: 0.8907773\n",
            "Time: 5.0378899574279785 seconds\n",
            "Time cumulative: 67.06117391586304 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 14 test rmse: 0.9366041 train rmse: 0.88289875\n",
            "Time: 4.36513352394104 seconds\n",
            "Time cumulative: 71.42630743980408 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 15 test rmse: 0.93498415 train rmse: 0.88005024\n",
            "Time: 4.593429327011108 seconds\n",
            "Time cumulative: 76.01973676681519 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 16 test rmse: 0.9318317 train rmse: 0.876458\n",
            "Time: 4.3305323123931885 seconds\n",
            "Time cumulative: 80.35026907920837 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 17 test rmse: 0.92902994 train rmse: 0.87176394\n",
            "Time: 4.877047777175903 seconds\n",
            "Time cumulative: 85.22731685638428 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 18 test rmse: 0.9266243 train rmse: 0.8682253\n",
            "Time: 4.924307346343994 seconds\n",
            "Time cumulative: 90.15162420272827 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 19 test rmse: 0.9251219 train rmse: 0.8654278\n",
            "Time: 4.264318943023682 seconds\n",
            "Time cumulative: 94.41594314575195 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 20 test rmse: 0.9237051 train rmse: 0.8636052\n",
            "Time: 4.464670419692993 seconds\n",
            "Time cumulative: 98.88061356544495 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 21 test rmse: 0.9230272 train rmse: 0.86109525\n",
            "Time: 4.287552833557129 seconds\n",
            "Time cumulative: 103.16816639900208 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 22 test rmse: 0.9201247 train rmse: 0.85819197\n",
            "Time: 4.592502117156982 seconds\n",
            "Time cumulative: 107.76066851615906 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 23 test rmse: 0.92096555 train rmse: 0.85493225\n",
            "Time: 7.278600692749023 seconds\n",
            "Time cumulative: 115.03926920890808 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 24 test rmse: 0.91509074 train rmse: 0.8492597\n",
            "Time: 4.7440338134765625 seconds\n",
            "Time cumulative: 119.78330302238464 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 25 test rmse: 0.9173997 train rmse: 0.8483816\n",
            "Time: 4.494723558425903 seconds\n",
            "Time cumulative: 124.27802658081055 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 26 test rmse: 0.91209155 train rmse: 0.84358734\n",
            "Time: 4.339847564697266 seconds\n",
            "Time cumulative: 128.6178741455078 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 27 test rmse: 0.9178484 train rmse: 0.8402485\n",
            "Time: 4.895761728286743 seconds\n",
            "Time cumulative: 133.51363587379456 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 28 test rmse: 0.9058919 train rmse: 0.82978517\n",
            "Time: 4.655538558959961 seconds\n",
            "Time cumulative: 138.16917443275452 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 29 test rmse: 0.9150925 train rmse: 0.82584375\n",
            "Time: 4.630679607391357 seconds\n",
            "Time cumulative: 142.79985404014587 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 30 test rmse: 0.90036976 train rmse: 0.8124445\n",
            "Time: 4.519728660583496 seconds\n",
            "Time cumulative: 147.31958270072937 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 1 test rmse: 0.92292994 test mae: 0.72089577 test ndcg: 0.8975810050195195\n",
            "Epoch: 1 train rmse: 0.81828105 train mae: 0.6388311 train ndcg: 0.922820628717853\n",
            "Time: 7.436856269836426 seconds\n",
            "Time cumulative: 154.7564389705658 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 2 test rmse: 0.90824026 test mae: 0.71151423 test ndcg: 0.8997612822921459\n",
            "Epoch: 2 train rmse: 0.79889494 train mae: 0.626413 train ndcg: 0.9268642883546246\n",
            "Time: 7.696311712265015 seconds\n",
            "Time cumulative: 162.4527506828308 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 3 test rmse: 0.90443474 test mae: 0.7077078 test ndcg: 0.901530423001825\n",
            "Epoch: 3 train rmse: 0.7910667 train mae: 0.62010926 train ndcg: 0.928638205146061\n",
            "Time: 10.541640043258667 seconds\n",
            "Time cumulative: 172.99439072608948 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 4 test rmse: 0.9027873 test mae: 0.70782405 test ndcg: 0.9017143457272863\n",
            "Epoch: 4 train rmse: 0.7861748 train mae: 0.6172016 train ndcg: 0.929405859738283\n",
            "Time: 7.76569676399231 seconds\n",
            "Time cumulative: 180.7600874900818 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 5 test rmse: 0.90034014 test mae: 0.70408654 test ndcg: 0.9013893050627324\n",
            "Epoch: 5 train rmse: 0.78247595 train mae: 0.6134033 train ndcg: 0.9301068033253209\n",
            "Time: 7.8665807247161865 seconds\n",
            "Time cumulative: 188.62666821479797 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 6 test rmse: 0.9003079 test mae: 0.7058972 test ndcg: 0.9024060078565433\n",
            "Epoch: 6 train rmse: 0.7791581 train mae: 0.61175334 train ndcg: 0.9307990319541608\n",
            "Time: 7.473539590835571 seconds\n",
            "Time cumulative: 196.10020780563354 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 7 test rmse: 0.8982435 test mae: 0.7027312 test ndcg: 0.902510395691748\n",
            "Epoch: 7 train rmse: 0.77650696 train mae: 0.6087937 train ndcg: 0.9314746562818969\n",
            "Time: 7.9904563426971436 seconds\n",
            "Time cumulative: 204.0906641483307 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 8 test rmse: 0.8976692 test mae: 0.7030326 test ndcg: 0.9032492801799973\n",
            "Epoch: 8 train rmse: 0.77401954 train mae: 0.60719687 train ndcg: 0.9320733753313198\n",
            "Time: 10.209364414215088 seconds\n",
            "Time cumulative: 214.30002856254578 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 9 test rmse: 0.89848346 test mae: 0.70316887 test ndcg: 0.903205559752328\n",
            "Epoch: 9 train rmse: 0.7705274 train mae: 0.60423166 train ndcg: 0.9329286998834163\n",
            "Time: 9.776202201843262 seconds\n",
            "Time cumulative: 224.07623076438904 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 10 test rmse: 0.8957993 test mae: 0.70164984 test ndcg: 0.9036893272413077\n",
            "Epoch: 10 train rmse: 0.76559085 train mae: 0.6006662 train ndcg: 0.9340268194372547\n",
            "Time: 7.779816150665283 seconds\n",
            "Time cumulative: 231.85604691505432 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 11 test rmse: 0.8958825 test mae: 0.70093364 test ndcg: 0.9035447292583734\n",
            "Epoch: 11 train rmse: 0.7639345 train mae: 0.59920186 train ndcg: 0.934403024552082\n",
            "Time: 8.016096115112305 seconds\n",
            "Time cumulative: 239.87214303016663 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 12 test rmse: 0.89545643 test mae: 0.7017608 test ndcg: 0.9037154906765734\n",
            "Epoch: 12 train rmse: 0.7619516 train mae: 0.5978214 train ndcg: 0.9347030661760474\n",
            "Time: 7.488481521606445 seconds\n",
            "Time cumulative: 247.36062455177307 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 13 test rmse: 0.8950252 test mae: 0.7008667 test ndcg: 0.9036553880690072\n",
            "Epoch: 13 train rmse: 0.7601428 train mae: 0.5963282 train ndcg: 0.9349842820572529\n",
            "Time: 7.804929494857788 seconds\n",
            "Time cumulative: 255.16555404663086 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 14 test rmse: 0.89599943 test mae: 0.7023103 test ndcg: 0.9034033491913992\n",
            "Epoch: 14 train rmse: 0.7582529 train mae: 0.59533864 train ndcg: 0.9358034335326108\n",
            "Time: 7.374351739883423 seconds\n",
            "Time cumulative: 262.5399057865143 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 15 test rmse: 0.8944987 test mae: 0.69997036 test ndcg: 0.9034928420296783\n",
            "Epoch: 15 train rmse: 0.7558031 train mae: 0.5926318 train ndcg: 0.9362700693233231\n",
            "Time: 7.758587598800659 seconds\n",
            "Time cumulative: 270.29849338531494 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 16 test rmse: 0.89555 test mae: 0.7020767 test ndcg: 0.9043611088404723\n",
            "Epoch: 16 train rmse: 0.75425303 train mae: 0.5921406 train ndcg: 0.936821811708806\n",
            "Time: 10.336531162261963 seconds\n",
            "Time cumulative: 280.6350245475769 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 17 test rmse: 0.89407384 test mae: 0.69950056 test ndcg: 0.9037308318288624\n",
            "Epoch: 17 train rmse: 0.7519636 train mae: 0.58946824 train ndcg: 0.9369633820620391\n",
            "Time: 7.402432203292847 seconds\n",
            "Time cumulative: 288.03745675086975 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 18 test rmse: 0.8953131 test mae: 0.70192957 test ndcg: 0.9042792988249866\n",
            "Epoch: 18 train rmse: 0.75031227 train mae: 0.5889611 train ndcg: 0.9374785501063151\n",
            "Time: 7.830188035964966 seconds\n",
            "Time cumulative: 295.8676447868347 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 19 test rmse: 0.89357746 test mae: 0.6990127 test ndcg: 0.904102999892357\n",
            "Epoch: 19 train rmse: 0.747829 train mae: 0.58602816 train ndcg: 0.9378922424508203\n",
            "Time: 7.441783666610718 seconds\n",
            "Time cumulative: 303.30942845344543 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 20 test rmse: 0.8951277 test mae: 0.70178 test ndcg: 0.9039634148767232\n",
            "Epoch: 20 train rmse: 0.74573034 train mae: 0.5850037 train ndcg: 0.938695595912166\n",
            "Time: 7.551004886627197 seconds\n",
            "Time cumulative: 310.86043334007263 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 21 test rmse: 0.893441 test mae: 0.69864386 test ndcg: 0.9038762673014444\n",
            "Epoch: 21 train rmse: 0.7429931 train mae: 0.58207244 train ndcg: 0.9389889809990286\n",
            "Time: 7.819143533706665 seconds\n",
            "Time cumulative: 318.6795768737793 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 22 test rmse: 0.89408034 test mae: 0.7005439 test ndcg: 0.9040207013126886\n",
            "Epoch: 22 train rmse: 0.7412144 train mae: 0.5810517 train ndcg: 0.939620871359397\n",
            "Time: 9.339281797409058 seconds\n",
            "Time cumulative: 328.01885867118835 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 23 test rmse: 0.89420825 test mae: 0.6992729 test ndcg: 0.9043769891668203\n",
            "Epoch: 23 train rmse: 0.7394836 train mae: 0.5793527 train ndcg: 0.9400822361160944\n",
            "Time: 7.8500611782073975 seconds\n",
            "Time cumulative: 335.86891984939575 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 24 test rmse: 0.8935345 test mae: 0.6990414 test ndcg: 0.9040025653847391\n",
            "Epoch: 24 train rmse: 0.73711795 train mae: 0.57710457 train ndcg: 0.9404171915082888\n",
            "Time: 7.701916933059692 seconds\n",
            "Time cumulative: 343.57083678245544 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 25 test rmse: 0.8935462 test mae: 0.6987747 test ndcg: 0.904596851860246\n",
            "Epoch: 25 train rmse: 0.73560566 train mae: 0.5760755 train ndcg: 0.9408251040923505\n",
            "Time: 7.529333829879761 seconds\n",
            "Time cumulative: 351.1001706123352 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 26 test rmse: 0.8933825 test mae: 0.69909 test ndcg: 0.9044962666069812\n",
            "Epoch: 26 train rmse: 0.7344785 train mae: 0.57522666 train ndcg: 0.9412757133499665\n",
            "Time: 7.598427772521973 seconds\n",
            "Time cumulative: 358.6985983848572 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 27 test rmse: 0.8939259 test mae: 0.6988894 test ndcg: 0.9039976585950427\n",
            "Epoch: 27 train rmse: 0.7332866 train mae: 0.5743663 train ndcg: 0.941625825647838\n",
            "Time: 7.773169994354248 seconds\n",
            "Time cumulative: 366.4717683792114 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 28 test rmse: 0.8934833 test mae: 0.69897825 test ndcg: 0.9042334106437416\n",
            "Epoch: 28 train rmse: 0.7316641 train mae: 0.57307917 train ndcg: 0.9419037330618104\n",
            "Time: 7.5168187618255615 seconds\n",
            "Time cumulative: 373.988587141037 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 29 test rmse: 0.8937372 test mae: 0.6985371 test ndcg: 0.9041858291879933\n",
            "Epoch: 29 train rmse: 0.7300413 train mae: 0.5716341 train ndcg: 0.9423802092524045\n",
            "Time: 10.209798097610474 seconds\n",
            "Time cumulative: 384.19838523864746 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 30 test rmse: 0.8936226 test mae: 0.6991817 test ndcg: 0.9048070127050846\n",
            "Epoch: 30 train rmse: 0.72868246 train mae: 0.57063705 train ndcg: 0.9427714834025804\n",
            "Time: 7.739108085632324 seconds\n",
            "Time cumulative: 391.9374933242798 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 31 test rmse: 0.89350003 test mae: 0.698682 test ndcg: 0.9045758285066469\n",
            "Epoch: 31 train rmse: 0.727373 train mae: 0.56961733 train ndcg: 0.9429391182950383\n",
            "Time: 7.408900737762451 seconds\n",
            "Time cumulative: 399.34639406204224 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 32 test rmse: 0.89369595 test mae: 0.69908315 test ndcg: 0.9044129060055102\n",
            "Epoch: 32 train rmse: 0.72611123 train mae: 0.56850606 train ndcg: 0.9436629872412452\n",
            "Time: 7.5399909019470215 seconds\n",
            "Time cumulative: 406.88638496398926 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 33 test rmse: 0.89332443 test mae: 0.6984541 test ndcg: 0.9038806795405568\n",
            "Epoch: 33 train rmse: 0.7244311 train mae: 0.5671997 train ndcg: 0.9437620682494505\n",
            "Time: 7.781132459640503 seconds\n",
            "Time cumulative: 414.66751742362976 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 34 test rmse: 0.89363897 test mae: 0.69887763 test ndcg: 0.9038860474703846\n",
            "Epoch: 34 train rmse: 0.7231997 train mae: 0.5660707 train ndcg: 0.943957020181814\n",
            "Time: 9.64279842376709 seconds\n",
            "Time cumulative: 424.31031584739685 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 35 test rmse: 0.8936243 test mae: 0.6986244 test ndcg: 0.9038275870045147\n",
            "Epoch: 35 train rmse: 0.7215868 train mae: 0.5651055 train ndcg: 0.9444887995615185\n",
            "Time: 10.459891319274902 seconds\n",
            "Time cumulative: 434.77020716667175 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 36 test rmse: 0.8939502 test mae: 0.6992704 test ndcg: 0.9041808330480292\n",
            "Epoch: 36 train rmse: 0.7202427 train mae: 0.56393903 train ndcg: 0.9447482215406673\n",
            "Time: 7.735868215560913 seconds\n",
            "Time cumulative: 442.50607538223267 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 37 test rmse: 0.8938972 test mae: 0.69866633 test ndcg: 0.9039626652391382\n",
            "Epoch: 37 train rmse: 0.71883935 train mae: 0.56290615 train ndcg: 0.944952775089717\n",
            "Time: 7.414297580718994 seconds\n",
            "Time cumulative: 449.92037296295166 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 38 test rmse: 0.8941867 test mae: 0.6993925 test ndcg: 0.9041862157680443\n",
            "Epoch: 38 train rmse: 0.7176518 train mae: 0.56186116 train ndcg: 0.9451856593783476\n",
            "Time: 7.569910526275635 seconds\n",
            "Time cumulative: 457.4902834892273 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 39 test rmse: 0.89419883 test mae: 0.6988416 test ndcg: 0.904410165480937\n",
            "Epoch: 39 train rmse: 0.71624225 train mae: 0.56079924 train ndcg: 0.9452198313666138\n",
            "Time: 7.7230141162872314 seconds\n",
            "Time cumulative: 465.2132976055145 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 40 test rmse: 0.894139 test mae: 0.6991933 test ndcg: 0.9040351568055505\n",
            "Epoch: 40 train rmse: 0.71465856 train mae: 0.5592777 train ndcg: 0.9458017020527726\n",
            "Time: 8.101248979568481 seconds\n",
            "Time cumulative: 473.314546585083 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 41 test rmse: 0.89459527 test mae: 0.69889855 test ndcg: 0.903664574278585\n",
            "Epoch: 41 train rmse: 0.7129989 train mae: 0.5580922 train ndcg: 0.9463645745557421\n",
            "Time: 8.173697710037231 seconds\n",
            "Time cumulative: 481.48824429512024 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 42 test rmse: 0.8939602 test mae: 0.6988292 test ndcg: 0.903799353562298\n",
            "Epoch: 42 train rmse: 0.7115941 train mae: 0.5570498 train ndcg: 0.9462741018031932\n",
            "Time: 9.4893159866333 seconds\n",
            "Time cumulative: 490.97756028175354 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 43 test rmse: 0.89427376 test mae: 0.69925094 test ndcg: 0.9041397330984917\n",
            "Epoch: 43 train rmse: 0.710814 train mae: 0.5567263 train ndcg: 0.9468062274951031\n",
            "Time: 7.398336172103882 seconds\n",
            "Time cumulative: 498.3758964538574 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 44 test rmse: 0.8941227 test mae: 0.69885856 test ndcg: 0.9038616036538415\n",
            "Epoch: 44 train rmse: 0.7097842 train mae: 0.55545217 train ndcg: 0.9468177442309547\n",
            "Time: 7.556635618209839 seconds\n",
            "Time cumulative: 505.93253207206726 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 45 test rmse: 0.894372 test mae: 0.69960886 test ndcg: 0.9041769524360762\n",
            "Epoch: 45 train rmse: 0.70887065 train mae: 0.55521166 train ndcg: 0.9472390882341521\n",
            "Time: 7.7271459102630615 seconds\n",
            "Time cumulative: 513.6596779823303 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 46 test rmse: 0.8941597 test mae: 0.6987366 test ndcg: 0.9034981174366162\n",
            "Epoch: 46 train rmse: 0.7078924 train mae: 0.55393314 train ndcg: 0.9472081213171687\n",
            "Time: 7.362513542175293 seconds\n",
            "Time cumulative: 521.0221915245056 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 47 test rmse: 0.8944211 test mae: 0.6993585 test ndcg: 0.9038917500994641\n",
            "Epoch: 47 train rmse: 0.70695716 train mae: 0.5534075 train ndcg: 0.9474761152933128\n",
            "Time: 7.558257818222046 seconds\n",
            "Time cumulative: 528.5804493427277 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 48 test rmse: 0.89487237 test mae: 0.6999183 test ndcg: 0.9038123361458796\n",
            "Epoch: 48 train rmse: 0.7059408 train mae: 0.5529911 train ndcg: 0.9477347816908063\n",
            "Time: 8.684512853622437 seconds\n",
            "Time cumulative: 537.2649621963501 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 49 test rmse: 0.8944475 test mae: 0.6991022 test ndcg: 0.9035169732775359\n",
            "Epoch: 49 train rmse: 0.70509714 train mae: 0.5516986 train ndcg: 0.9477845923585955\n",
            "Time: 8.655317544937134 seconds\n",
            "Time cumulative: 545.9202797412872 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 50 test rmse: 0.894521 test mae: 0.69919217 test ndcg: 0.9034522305680394\n",
            "Epoch: 50 train rmse: 0.7041359 train mae: 0.5510766 train ndcg: 0.9482421982087482\n",
            "Time: 7.519123554229736 seconds\n",
            "Time cumulative: 553.439403295517 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 51 test rmse: 0.8948477 test mae: 0.699425 test ndcg: 0.9039121246417253\n",
            "Epoch: 51 train rmse: 0.70278066 train mae: 0.5499561 train ndcg: 0.9481553444794456\n",
            "Time: 7.6836206912994385 seconds\n",
            "Time cumulative: 561.1230239868164 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 52 test rmse: 0.8947991 test mae: 0.69920194 test ndcg: 0.9039034381460287\n",
            "Epoch: 52 train rmse: 0.7017642 train mae: 0.54905903 train ndcg: 0.9485529328331865\n",
            "Time: 7.377851963043213 seconds\n",
            "Time cumulative: 568.5008759498596 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 53 test rmse: 0.8949217 test mae: 0.69950795 test ndcg: 0.9037673937325166\n",
            "Epoch: 53 train rmse: 0.70080173 train mae: 0.548448 train ndcg: 0.9490549419817339\n",
            "Time: 7.641448020935059 seconds\n",
            "Time cumulative: 576.1423239707947 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 54 test rmse: 0.89515966 test mae: 0.69962174 test ndcg: 0.9036530253376508\n",
            "Epoch: 54 train rmse: 0.69965684 train mae: 0.5474972 train ndcg: 0.9489090516925247\n",
            "Time: 8.584749221801758 seconds\n",
            "Time cumulative: 584.7270731925964 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 55 test rmse: 0.8950043 test mae: 0.6996156 test ndcg: 0.9038526784565987\n",
            "Epoch: 55 train rmse: 0.698319 train mae: 0.5464983 train ndcg: 0.9492533376529436\n",
            "Time: 10.846974611282349 seconds\n",
            "Time cumulative: 595.5740478038788 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 56 test rmse: 0.89537156 test mae: 0.7001949 test ndcg: 0.9037175073919148\n",
            "Epoch: 56 train rmse: 0.69677216 train mae: 0.5456058 train ndcg: 0.9495204973544313\n",
            "Time: 7.589582920074463 seconds\n",
            "Time cumulative: 603.1636307239532 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 57 test rmse: 0.8954863 test mae: 0.6999824 test ndcg: 0.9038708903788008\n",
            "Epoch: 57 train rmse: 0.6954076 train mae: 0.54419327 train ndcg: 0.9496737832450103\n",
            "Time: 8.382696151733398 seconds\n",
            "Time cumulative: 611.5463268756866 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 58 test rmse: 0.895263 test mae: 0.69986635 test ndcg: 0.9036629334214555\n",
            "Epoch: 58 train rmse: 0.6945035 train mae: 0.54350716 train ndcg: 0.9501931868595098\n",
            "Time: 7.991933107376099 seconds\n",
            "Time cumulative: 619.5382599830627 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 59 test rmse: 0.8957872 test mae: 0.7007622 test ndcg: 0.9041001972239978\n",
            "Epoch: 59 train rmse: 0.69371635 train mae: 0.5433715 train ndcg: 0.9499623834271035\n",
            "Time: 10.966094017028809 seconds\n",
            "Time cumulative: 630.5043540000916 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 60 test rmse: 0.8954857 test mae: 0.70010257 test ndcg: 0.9035736385229624\n",
            "Epoch: 60 train rmse: 0.6926766 train mae: 0.5422188 train ndcg: 0.9502988818102498\n",
            "Time: 7.691762685775757 seconds\n",
            "Time cumulative: 638.1961166858673 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTi_PdXJqTjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056d3e51-502a-4c57-ed5f-d9f4816aa0b2"
      },
      "source": [
        "# Final result\n",
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 33  best rmse: 0.89332443\n",
            "Epoch: 33  best mae: 0.6984541\n",
            "Epoch: 30  best ndcg: 0.9048070127050846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qf9cvmAvxGx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}