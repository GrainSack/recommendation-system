{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GrainSack/recommendation-system/blob/main/glocal_k_with_ntk_11_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty3gYQgtnwFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1448cf-1dc5-4653-bfb3-39ae436942ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9L3y-n0wU5F",
        "outputId": "1a22ee98-3eb9-458f-f6a4-6410296788f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.9.2\n",
            "Uninstalling tensorflow-2.9.2:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.9.2.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McNeEqRswb1k",
        "outputId": "250a4b00-4a63-465f-d864-918e87a2d750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 25 kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.3.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.50.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.19.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.38.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=4cf64c7a4475c52a83eaab9acaf3a79c3a1cd126322d02bea1ec4b8fcc76b05e\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.17.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2tU6kL8Ot3"
      },
      "source": [
        "from time import time\n",
        "from scipy.sparse import csc_matrix\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcyWCv_WvSKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bcc959-1fb7-44fd-d095-405a27240e8b"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "ki6rqtDmqyhd",
        "outputId": "10a5a4c8-bc66-42eb-f0c4-a05343aec702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4A9uU1WloQ2"
      },
      "source": [
        "# Data Loader Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3KEUaVo1o3"
      },
      "source": [
        "def load_data_100k(path='./content/drive/MyDrive/MovieLens_100K', delimiter='\\t'):\n",
        "\n",
        "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    total = np.concatenate((train, test), axis=0)\n",
        "\n",
        "    n_u = np.unique(total[:,0]).size  # num of users\n",
        "    n_m = np.unique(total[:,1]).size  # num of movies\n",
        "    n_train = train.shape[0]  # num of training ratings\n",
        "    n_test = test.shape[0]  # num of test ratings\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_train):\n",
        "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
        "\n",
        "    for i in range(n_test):\n",
        "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3e8Xg3us8g7"
      },
      "source": [
        "def load_data_1m(path='./content/drive/MyDrive/MovieLens_1M', delimiter='::', frac=0.1, seed=1234):\n",
        "\n",
        "    tic = time()\n",
        "    print('reading data...')\n",
        "    data = np.loadtxt(path+'movielens_1m_dataset.dat', skiprows=0, delimiter=delimiter).astype('int32')\n",
        "    print('taken', time() - tic, 'seconds')\n",
        "\n",
        "    n_u = np.unique(data[:,0]).size  # num of users\n",
        "    n_m = np.unique(data[:,1]).size  # num of movies\n",
        "    n_r = data.shape[0]  # num of ratings\n",
        "\n",
        "    udict = {}\n",
        "    for i, u in enumerate(np.unique(data[:,0]).tolist()):\n",
        "        udict[u] = i\n",
        "    mdict = {}\n",
        "    for i, m in enumerate(np.unique(data[:,1]).tolist()):\n",
        "        mdict[m] = i\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    idx = np.arange(n_r)\n",
        "    np.random.shuffle(idx)\n",
        "\n",
        "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
        "\n",
        "    for i in range(n_r):\n",
        "        u_id = data[idx[i], 0]\n",
        "        m_id = data[idx[i], 1]\n",
        "        r = data[idx[i], 2]\n",
        "\n",
        "        if i < int(frac * n_r):\n",
        "            test_r[mdict[m_id], udict[u_id]] = r\n",
        "        else:\n",
        "            train_r[mdict[m_id], udict[u_id]] = r\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_r - int(frac * n_r)))\n",
        "    print('num of test ratings: {}'.format(int(frac * n_r)))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rMjcbLvhtRs"
      },
      "source": [
        "def load_matlab_file(path_file, name_field):\n",
        "    \n",
        "    db = h5py.File(path_file, 'r')\n",
        "    ds = db[name_field]\n",
        "\n",
        "    try:\n",
        "        if 'ir' in ds.keys():\n",
        "            data = np.asarray(ds['data'])\n",
        "            ir   = np.asarray(ds['ir'])\n",
        "            jc   = np.asarray(ds['jc'])\n",
        "            out  = csc_matrix((data, ir, jc)).astype(np.float32)\n",
        "    except AttributeError:\n",
        "        out = np.asarray(ds).astype(np.float32).T\n",
        "\n",
        "    db.close()\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6pIUrkza2zv"
      },
      "source": [
        "def load_data_monti(path='./'):\n",
        "\n",
        "    M = load_matlab_file(path+'douban_monti_dataset.mat', 'M')\n",
        "    Otraining = load_matlab_file(path+'douban_monti_dataset.mat', 'Otraining') * M\n",
        "    Otest = load_matlab_file(path+'douban_monti_dataset.mat', 'Otest') * M\n",
        "\n",
        "    n_u = M.shape[0]  # num of users\n",
        "    n_m = M.shape[1]  # num of movies\n",
        "    n_train = Otraining[np.where(Otraining)].size  # num of training ratings\n",
        "    n_test = Otest[np.where(Otest)].size  # num of test ratings\n",
        "\n",
        "    train_r = Otraining.T\n",
        "    test_r = Otest.T\n",
        "\n",
        "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
        "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
        "\n",
        "    print('data matrix loaded')\n",
        "    print('num of users: {}'.format(n_u))\n",
        "    print('num of movies: {}'.format(n_m))\n",
        "    print('num of training ratings: {}'.format(n_train))\n",
        "    print('num of test ratings: {}'.format(n_test))\n",
        "\n",
        "    return n_m, n_u, train_r, train_m, test_r, test_m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8kEkg9mlIW"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fkA1WpmipzF"
      },
      "source": [
        "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "data_path = '/content/drive/'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijlu0lXQioYM"
      },
      "source": [
        "# Select a dataset among 'ML-1M', 'ML-100K', and 'Douban'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
        "dataset = 'ML-1M'\n",
        "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqSSY33mgkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b71d7a-7e88-43a1-fe2e-90436537aa50"
      },
      "source": [
        "# Data Load\n",
        "try:\n",
        "    if dataset == 'ML-100K':\n",
        "        path = '/content/drive/MyDrive/MovieLens_100K/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')\n",
        "\n",
        "    elif dataset == 'ML-1M':\n",
        "        path = '/content/drive/MyDrive/MovieLens_1M/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_1m(path=path, delimiter='::', frac=0.1, seed=1234)\n",
        "\n",
        "    elif dataset == 'Douban':\n",
        "        path = data_path + 'MyDrive/Douban_monti/'\n",
        "        n_m, n_u, train_r, train_m, test_r, test_m = load_data_monti(path=path)\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "except ValueError:\n",
        "    print('Error: Unable to load data')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading data...\n",
            "taken 6.603676080703735 seconds\n",
            "data matrix loaded\n",
            "num of users: 6040\n",
            "num of movies: 3706\n",
            "num of training ratings: 900189\n",
            "num of test ratings: 100020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQMtA9yml-gp"
      },
      "source": [
        "# Hyperparameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGCdp_FlobOK"
      },
      "source": [
        "# Common hyperparameter settings\n",
        "n_hid = 500\n",
        "n_dim = 5\n",
        "n_layers = 2\n",
        "gk_size = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "344bwGB0cWXp"
      },
      "source": [
        "# Different hyperparameter settings for each dataset\n",
        "if dataset == 'ML-100K':\n",
        "    lambda_2 = 20.  # l2 regularisation\n",
        "    lambda_s = 0.006\n",
        "    iter_p = 5  # optimisation\n",
        "    iter_f = 5\n",
        "    epoch_p = 30  # training epoch\n",
        "    epoch_f = 60\n",
        "    dot_scale = 1  # scaled dot product\n",
        "\n",
        "elif dataset == 'ML-1M':\n",
        "    lambda_2 = 70.\n",
        "    lambda_s = 0.018\n",
        "    iter_p = 50\n",
        "    iter_f = 10\n",
        "    epoch_p = 20\n",
        "    epoch_f = 30\n",
        "    dot_scale = 0.5\n",
        "\n",
        "elif dataset == 'Douban':\n",
        "    lambda_2 = 10.\n",
        "    lambda_s = 0.022\n",
        "    iter_p = 5\n",
        "    iter_f = 5\n",
        "    epoch_p = 20\n",
        "    epoch_f = 60\n",
        "    dot_scale = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94aimX3nAMI"
      },
      "source": [
        "R = tf.placeholder(\"float\", [n_m, n_u])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sWtU4-pmDDT"
      },
      "source": [
        "# Network Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX2wREO09zde"
      },
      "source": [
        "def local_kernel(u, v):\n",
        "\n",
        "    dist = tf.norm(u - v, ord=2, axis=2)\n",
        "    hat = tf.maximum(0., 1. - dist**2)\n",
        "\n",
        "    return hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c88l9LYr9175"
      },
      "source": [
        "def kernel_layer(x, n_hid=n_hid, n_dim=n_dim, activation=tf.nn.sigmoid, lambda_s=lambda_s, lambda_2=lambda_2, name=''):\n",
        "\n",
        "    with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
        "        W = tf.get_variable('W', [x.shape[1], n_hid])\n",
        "        n_in = x.get_shape().as_list()[1]\n",
        "        u = tf.get_variable('u', initializer=tf.random.truncated_normal([n_in, 1, n_dim], 0., 1e-3))\n",
        "        v = tf.get_variable('v', initializer=tf.random.truncated_normal([1, n_hid, n_dim], 0., 1e-3))\n",
        "        b = tf.get_variable('b', [n_hid])\n",
        "\n",
        "    w_hat = local_kernel(u, v)\n",
        "    \n",
        "    sparse_reg = tf.contrib.layers.l2_regularizer(lambda_s)\n",
        "    sparse_reg_term = tf.contrib.layers.apply_regularization(sparse_reg, [w_hat])\n",
        "    \n",
        "    l2_reg = tf.contrib.layers.l2_regularizer(lambda_2)\n",
        "    l2_reg_term = tf.contrib.layers.apply_regularization(l2_reg, [W])\n",
        "\n",
        "    W_eff = W * w_hat  # Local kernelised weight matrix\n",
        "    y = tf.matmul(x, W_eff) + b\n",
        "    y = activation(y)\n",
        "\n",
        "    return y, sparse_reg_term + l2_reg_term"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlb95FmRVATa"
      },
      "source": [
        "def global_kernel(input, gk_size, dot_scale):\n",
        "\n",
        "    avg_pooling = tf.reduce_mean(input, axis=1)  # Item (axis=1) based average pooling\n",
        "    avg_pooling = tf.reshape(avg_pooling, [1, -1])\n",
        "    n_kernel = avg_pooling.shape[1].value\n",
        "\n",
        "    conv_kernel = tf.get_variable('conv_kernel', initializer=tf.random.truncated_normal([n_kernel, gk_size**2], stddev=0.1))\n",
        "    gk = tf.matmul(avg_pooling, conv_kernel) * dot_scale  # Scaled dot product\n",
        "    gk = tf.reshape(gk, [gk_size, gk_size, 1, 1])\n",
        "\n",
        "    return gk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTLi_65XzIbH"
      },
      "source": [
        "def global_conv(input, W):\n",
        "\n",
        "    input = tf.reshape(input, [1, input.shape[0], input.shape[1], 1])\n",
        "    conv2d = tf.nn.relu(tf.nn.conv2d(input, W, strides=[1,1,1,1], padding='SAME'))\n",
        "\n",
        "    return tf.reshape(conv2d, [conv2d.shape[1], conv2d.shape[2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8sQCwrSmKG4"
      },
      "source": [
        "# Network Instantiation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtWj1SCo1RW"
      },
      "source": [
        "## Pre-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7teUrgWagpW0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ee5dd6-457f-4091-cb07-48ef1ffe581c"
      },
      "source": [
        "y = R\n",
        "reg_losses = None\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y, reg_loss = kernel_layer(y, name=str(i))\n",
        "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "pred_p, reg_loss = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
        "reg_losses = reg_losses + reg_loss\n",
        "\n",
        "# L2 loss\n",
        "diff = train_m * (train_r - pred_p)\n",
        "sqE = tf.nn.l2_loss(diff)\n",
        "loss_p = sqE + reg_losses\n",
        "\n",
        "optimizer_p = tf.contrib.opt.ScipyOptimizerInterface(loss_p, options={'disp': True, 'maxiter': iter_p, 'maxcor': 10}, method='L-BFGS-B')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IEBsNhNo4Cj"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiTXqnN6zLXQ"
      },
      "source": [
        "y = R\n",
        "reg_losses = None\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y, _ = kernel_layer(y, name=str(i))\n",
        "\n",
        "y_dash, _ = kernel_layer(y, n_u, activation=tf.identity, name='out')\n",
        "\n",
        "gk = global_kernel(y_dash, gk_size, dot_scale)  # Global kernel\n",
        "y_hat = global_conv(train_r, gk)  # Global kernel-based rating matrix\n",
        "\n",
        "for i in range(n_layers):\n",
        "    y_hat, reg_loss = kernel_layer(y_hat, name=str(i))\n",
        "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
        "\n",
        "pred_f, reg_loss = kernel_layer(y_hat, n_u, activation=tf.identity, name='out')\n",
        "reg_losses = reg_losses + reg_loss\n",
        "\n",
        "# L2 loss\n",
        "diff = train_m * (train_r - pred_f)\n",
        "sqE = tf.nn.l2_loss(diff)\n",
        "loss_f = sqE + reg_losses\n",
        "\n",
        "optimizer_f = tf.contrib.opt.ScipyOptimizerInterface(loss_f, options={'disp': True, 'maxiter': iter_f, 'maxcor': 10}, method='L-BFGS-B')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation code"
      ],
      "metadata": {
        "id": "sETwz58aK6y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dcg_k(score_label, k):\n",
        "    dcg, i = 0., 0\n",
        "    for s in score_label:\n",
        "        if i < k:\n",
        "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    return dcg"
      ],
      "metadata": {
        "id": "vyReXxgac3KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ndcg_k(y_hat, y, k):\n",
        "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
        "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
        "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
        "    norm, i = 0., 0\n",
        "    for s in score_label_:\n",
        "        if i < k:\n",
        "            norm += (2**s[1]-1) / np.log2(2+i)\n",
        "            i += 1\n",
        "    dcg = dcg_k(score_label, k)\n",
        "    return dcg / norm"
      ],
      "metadata": {
        "id": "jwsSR-8ZdGWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_ndcg(y_hat, y):\n",
        "    ndcg_sum, num = 0, 0\n",
        "    y_hat, y = y_hat.T, y.T\n",
        "    n_users = y.shape[0]\n",
        "\n",
        "    for i in range(n_users):\n",
        "        y_hat_i = y_hat[i][np.where(y[i])]\n",
        "        y_i = y[i][np.where(y[i])]\n",
        "\n",
        "        if y_i.shape[0] < 2:\n",
        "            continue\n",
        "\n",
        "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
        "        num += 1\n",
        "\n",
        "    return ndcg_sum / num"
      ],
      "metadata": {
        "id": "yy9eQS51pbhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXXQjeMxmYEC"
      },
      "source": [
        "# Training and Test Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ35Zoha-Eue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76e4529-ed40-4b13-8a34-847dc2e88d1d"
      },
      "source": [
        "best_rmse_ep, best_mae_ep, best_ndcg_ep = 0, 0, 0\n",
        "best_rmse, best_mae, best_ndcg = float(\"inf\"), float(\"inf\"), 0\n",
        "\n",
        "time_cumulative = 0\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for i in range(epoch_p):\n",
        "        tic = time()\n",
        "        optimizer_p.minimize(sess, feed_dict={R: train_r})\n",
        "        pre = sess.run(pred_p, feed_dict={R: train_r})\n",
        "\n",
        "        t = time() - tic\n",
        "        time_cumulative += t\n",
        "        \n",
        "        error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "        test_rmse = np.sqrt(error)\n",
        "\n",
        "        error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "        train_rmse = np.sqrt(error_train)\n",
        "\n",
        "        print('.-^-._' * 12)\n",
        "        print('PRE-TRAINING')\n",
        "        print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
        "        print('Time:', t, 'seconds')\n",
        "        print('Time cumulative:', time_cumulative, 'seconds')\n",
        "        print('.-^-._' * 12)\n",
        "\n",
        "    for i in range(epoch_f):\n",
        "        tic = time()\n",
        "        optimizer_f.minimize(sess, feed_dict={R: train_r})\n",
        "        pre = sess.run(pred_f, feed_dict={R: train_r})\n",
        "\n",
        "        t = time() - tic\n",
        "        time_cumulative += t\n",
        "        \n",
        "        error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
        "        test_rmse = np.sqrt(error)\n",
        "\n",
        "        error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
        "        train_rmse = np.sqrt(error_train)\n",
        "\n",
        "        test_mae = (test_m * np.abs(np.clip(pre, 1., 5.) - test_r)).sum() / test_m.sum()\n",
        "        train_mae = (train_m * np.abs(np.clip(pre, 1., 5.) - train_r)).sum() / train_m.sum()\n",
        "\n",
        "        test_ndcg = call_ndcg(np.clip(pre, 1., 5.), test_r)\n",
        "        train_ndcg = call_ndcg(np.clip(pre, 1., 5.), train_r)\n",
        "\n",
        "        if test_rmse < best_rmse:\n",
        "            best_rmse = test_rmse\n",
        "            best_rmse_ep = i+1\n",
        "\n",
        "        if test_mae < best_mae:\n",
        "            best_mae = test_mae\n",
        "            best_mae_ep = i+1\n",
        "\n",
        "        if best_ndcg < test_ndcg:\n",
        "            best_ndcg = test_ndcg\n",
        "            best_ndcg_ep = i+1\n",
        "\n",
        "        print('.-^-._' * 12)\n",
        "        print('FINE-TUNING')\n",
        "        print('Epoch:', i+1, 'test rmse:', test_rmse, 'test mae:', test_mae, 'test ndcg:', test_ndcg)\n",
        "        print('Epoch:', i+1, 'train rmse:', train_rmse, 'train mae:', train_mae, 'train ndcg:', train_ndcg)\n",
        "        print('Time:', t, 'seconds')\n",
        "        print('Time cumulative:', time_cumulative, 'seconds')\n",
        "        print('.-^-._' * 12)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 1 test rmse: 0.886504 train rmse: 0.8716009\n",
            "Time: 200.3262574672699 seconds\n",
            "Time cumulative: 200.3262574672699 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 2 test rmse: 0.8534682 train rmse: 0.8168131\n",
            "Time: 182.8992531299591 seconds\n",
            "Time cumulative: 383.225510597229 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 3 test rmse: 0.84039575 train rmse: 0.78886414\n",
            "Time: 186.34759187698364 seconds\n",
            "Time cumulative: 569.5731024742126 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 4 test rmse: 0.83420444 train rmse: 0.76699173\n",
            "Time: 180.38577437400818 seconds\n",
            "Time cumulative: 749.9588768482208 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 5 test rmse: 0.83252114 train rmse: 0.75862294\n",
            "Time: 184.0825023651123 seconds\n",
            "Time cumulative: 934.0413792133331 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 6 test rmse: 0.8304148 train rmse: 0.7521848\n",
            "Time: 179.3745937347412 seconds\n",
            "Time cumulative: 1113.4159729480743 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 7 test rmse: 0.828578 train rmse: 0.74518657\n",
            "Time: 182.03689336776733 seconds\n",
            "Time cumulative: 1295.4528663158417 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 8 test rmse: 0.82772166 train rmse: 0.7403288\n",
            "Time: 182.94041991233826 seconds\n",
            "Time cumulative: 1478.39328622818 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 9 test rmse: 0.8263137 train rmse: 0.73625404\n",
            "Time: 180.89214277267456 seconds\n",
            "Time cumulative: 1659.2854290008545 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 10 test rmse: 0.82530075 train rmse: 0.73141676\n",
            "Time: 179.68259119987488 seconds\n",
            "Time cumulative: 1838.9680202007294 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 11 test rmse: 0.82467186 train rmse: 0.72954404\n",
            "Time: 176.11553597450256 seconds\n",
            "Time cumulative: 2015.083556175232 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 12 test rmse: 0.8242712 train rmse: 0.7266721\n",
            "Time: 177.56166887283325 seconds\n",
            "Time cumulative: 2192.645225048065 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 13 test rmse: 0.8236694 train rmse: 0.72537214\n",
            "Time: 179.1981017589569 seconds\n",
            "Time cumulative: 2371.843326807022 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 14 test rmse: 0.8237803 train rmse: 0.7234741\n",
            "Time: 178.250385761261 seconds\n",
            "Time cumulative: 2550.093712568283 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 15 test rmse: 0.8233505 train rmse: 0.72198784\n",
            "Time: 178.73150944709778 seconds\n",
            "Time cumulative: 2728.825222015381 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 16 test rmse: 0.8233733 train rmse: 0.7200328\n",
            "Time: 199.50883269309998 seconds\n",
            "Time cumulative: 2928.334054708481 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 17 test rmse: 0.82304245 train rmse: 0.7185409\n",
            "Time: 196.80805373191833 seconds\n",
            "Time cumulative: 3125.142108440399 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 18 test rmse: 0.82325125 train rmse: 0.7168891\n",
            "Time: 188.62490248680115 seconds\n",
            "Time cumulative: 3313.7670109272003 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 19 test rmse: 0.8230674 train rmse: 0.7155722\n",
            "Time: 186.29112339019775 seconds\n",
            "Time cumulative: 3500.058134317398 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "PRE-TRAINING\n",
            "Epoch: 20 test rmse: 0.82336736 train rmse: 0.71415913\n",
            "Time: 176.36256432533264 seconds\n",
            "Time cumulative: 3676.4206986427307 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 1 test rmse: 0.83153 test mae: 0.6474551 test ndcg: 0.9273839738436374\n",
            "Epoch: 1 train rmse: 0.71529263 train mae: 0.55924904 train ndcg: 0.9637664234729493\n",
            "Time: 115.49765944480896 seconds\n",
            "Time cumulative: 3791.9183580875397 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 2 test rmse: 0.8266466 test mae: 0.64343953 test ndcg: 0.9286174042516194\n",
            "Epoch: 2 train rmse: 0.7063238 train mae: 0.55220246 train ndcg: 0.9647986727360863\n",
            "Time: 111.85367727279663 seconds\n",
            "Time cumulative: 3903.7720353603363 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 3 test rmse: 0.8251425 test mae: 0.6432003 test ndcg: 0.9283736842096206\n",
            "Epoch: 3 train rmse: 0.70368165 train mae: 0.55117196 train ndcg: 0.9652485805968357\n",
            "Time: 117.60529279708862 seconds\n",
            "Time cumulative: 4021.377328157425 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 4 test rmse: 0.82432866 test mae: 0.642231 test ndcg: 0.9286855257996152\n",
            "Epoch: 4 train rmse: 0.70185834 train mae: 0.5495035 train ndcg: 0.9654593740947276\n",
            "Time: 117.58298349380493 seconds\n",
            "Time cumulative: 4138.96031165123 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 5 test rmse: 0.8239871 test mae: 0.642362 test ndcg: 0.9284212328073699\n",
            "Epoch: 5 train rmse: 0.70067596 train mae: 0.54883677 train ndcg: 0.9656280035801037\n",
            "Time: 113.48442816734314 seconds\n",
            "Time cumulative: 4252.444739818573 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 6 test rmse: 0.82360905 test mae: 0.6418619 test ndcg: 0.9284548712753594\n",
            "Epoch: 6 train rmse: 0.6999298 train mae: 0.54814553 train ndcg: 0.9657852899133436\n",
            "Time: 122.31532454490662 seconds\n",
            "Time cumulative: 4374.76006436348 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 7 test rmse: 0.8233977 test mae: 0.6419115 test ndcg: 0.9283657066244494\n",
            "Epoch: 7 train rmse: 0.699167 train mae: 0.5477591 train ndcg: 0.9659227783687638\n",
            "Time: 113.2023286819458 seconds\n",
            "Time cumulative: 4487.962393045425 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 8 test rmse: 0.8233928 test mae: 0.64171374 test ndcg: 0.9284430861405466\n",
            "Epoch: 8 train rmse: 0.6985901 train mae: 0.5472212 train ndcg: 0.965971883489427\n",
            "Time: 111.9068648815155 seconds\n",
            "Time cumulative: 4599.869257926941 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 9 test rmse: 0.82328796 test mae: 0.6418087 test ndcg: 0.9285306558117165\n",
            "Epoch: 9 train rmse: 0.6982532 train mae: 0.5470559 train ndcg: 0.9660564418233923\n",
            "Time: 123.53637742996216 seconds\n",
            "Time cumulative: 4723.405635356903 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 10 test rmse: 0.823413 test mae: 0.6417933 test ndcg: 0.9285945125265227\n",
            "Epoch: 10 train rmse: 0.69783527 train mae: 0.546733 train ndcg: 0.9660733823305698\n",
            "Time: 115.79578638076782 seconds\n",
            "Time cumulative: 4839.201421737671 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 11 test rmse: 0.82329863 test mae: 0.6417489 test ndcg: 0.9284219526442516\n",
            "Epoch: 11 train rmse: 0.6974423 train mae: 0.5464389 train ndcg: 0.9661283445328079\n",
            "Time: 116.18133568763733 seconds\n",
            "Time cumulative: 4955.382757425308 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 12 test rmse: 0.8231485 test mae: 0.64170504 test ndcg: 0.9283880931512354\n",
            "Epoch: 12 train rmse: 0.6969828 train mae: 0.5461726 train ndcg: 0.9661778998484614\n",
            "Time: 122.9736921787262 seconds\n",
            "Time cumulative: 5078.356449604034 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 13 test rmse: 0.8230737 test mae: 0.6416096 test ndcg: 0.9284299610358486\n",
            "Epoch: 13 train rmse: 0.69674224 train mae: 0.5459481 train ndcg: 0.966256149474659\n",
            "Time: 111.24062609672546 seconds\n",
            "Time cumulative: 5189.59707570076 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 14 test rmse: 0.8230247 test mae: 0.64151555 test ndcg: 0.928475496562513\n",
            "Epoch: 14 train rmse: 0.69641733 train mae: 0.5457297 train ndcg: 0.9662567168054303\n",
            "Time: 113.49313855171204 seconds\n",
            "Time cumulative: 5303.090214252472 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 15 test rmse: 0.8230669 test mae: 0.64165676 test ndcg: 0.9284579198825086\n",
            "Epoch: 15 train rmse: 0.6960811 train mae: 0.54548347 train ndcg: 0.9662651203315392\n",
            "Time: 118.76948833465576 seconds\n",
            "Time cumulative: 5421.859702587128 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 16 test rmse: 0.822987 test mae: 0.6414332 test ndcg: 0.9284981252212907\n",
            "Epoch: 16 train rmse: 0.695878 train mae: 0.54526556 train ndcg: 0.9663142512017932\n",
            "Time: 119.05090093612671 seconds\n",
            "Time cumulative: 5540.910603523254 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 17 test rmse: 0.82301044 test mae: 0.64156014 test ndcg: 0.9284126758338742\n",
            "Epoch: 17 train rmse: 0.6955907 train mae: 0.5450693 train ndcg: 0.9663466952763548\n",
            "Time: 114.36427974700928 seconds\n",
            "Time cumulative: 5655.274883270264 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 18 test rmse: 0.8231201 test mae: 0.64169437 test ndcg: 0.9285236768353445\n",
            "Epoch: 18 train rmse: 0.6953569 train mae: 0.5449877 train ndcg: 0.9663581172722242\n",
            "Time: 118.8693699836731 seconds\n",
            "Time cumulative: 5774.144253253937 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 19 test rmse: 0.8230283 test mae: 0.6415872 test ndcg: 0.928541742379025\n",
            "Epoch: 19 train rmse: 0.6949919 train mae: 0.5446292 train ndcg: 0.9664427458287884\n",
            "Time: 121.10127639770508 seconds\n",
            "Time cumulative: 5895.245529651642 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 20 test rmse: 0.82306737 test mae: 0.6416353 test ndcg: 0.9284669047376466\n",
            "Epoch: 20 train rmse: 0.6947361 train mae: 0.54449844 train ndcg: 0.9664525237808761\n",
            "Time: 115.19895529747009 seconds\n",
            "Time cumulative: 6010.444484949112 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 21 test rmse: 0.8230583 test mae: 0.6416638 test ndcg: 0.9286088025880287\n",
            "Epoch: 21 train rmse: 0.69452626 train mae: 0.5443137 train ndcg: 0.9664771223012577\n",
            "Time: 118.69056534767151 seconds\n",
            "Time cumulative: 6129.135050296783 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 22 test rmse: 0.82315147 test mae: 0.6418001 test ndcg: 0.9285160970864922\n",
            "Epoch: 22 train rmse: 0.694296 train mae: 0.54425025 train ndcg: 0.9664987943551834\n",
            "Time: 120.79983496665955 seconds\n",
            "Time cumulative: 6249.934885263443 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 23 test rmse: 0.8229355 test mae: 0.6414849 test ndcg: 0.9285644899979999\n",
            "Epoch: 23 train rmse: 0.69413716 train mae: 0.54392093 train ndcg: 0.9665148889953689\n",
            "Time: 122.31972122192383 seconds\n",
            "Time cumulative: 6372.254606485367 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 24 test rmse: 0.8230164 test mae: 0.6415878 test ndcg: 0.9286085233179072\n",
            "Epoch: 24 train rmse: 0.6939475 train mae: 0.54389626 train ndcg: 0.966539101359561\n",
            "Time: 120.27340650558472 seconds\n",
            "Time cumulative: 6492.5280129909515 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 25 test rmse: 0.8230126 test mae: 0.64161277 test ndcg: 0.9285651609681446\n",
            "Epoch: 25 train rmse: 0.69375724 train mae: 0.54373497 train ndcg: 0.9665707127868269\n",
            "Time: 117.91361880302429 seconds\n",
            "Time cumulative: 6610.441631793976 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 26 test rmse: 0.82317656 test mae: 0.64178425 test ndcg: 0.9286644898891101\n",
            "Epoch: 26 train rmse: 0.6936376 train mae: 0.54378265 train ndcg: 0.966593416421156\n",
            "Time: 121.3328275680542 seconds\n",
            "Time cumulative: 6731.77445936203 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 27 test rmse: 0.82302314 test mae: 0.6415744 test ndcg: 0.928699194902194\n",
            "Epoch: 27 train rmse: 0.6933358 train mae: 0.5433949 train ndcg: 0.966627031766525\n",
            "Time: 122.22558236122131 seconds\n",
            "Time cumulative: 6854.000041723251 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 28 test rmse: 0.8231446 test mae: 0.6416377 test ndcg: 0.9286925444595664\n",
            "Epoch: 28 train rmse: 0.6931585 train mae: 0.543299 train ndcg: 0.9666525993100503\n",
            "Time: 113.73465538024902 seconds\n",
            "Time cumulative: 6967.7346971035 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 29 test rmse: 0.8230494 test mae: 0.6415714 test ndcg: 0.9286796603966692\n",
            "Epoch: 29 train rmse: 0.6929084 train mae: 0.54304355 train ndcg: 0.966711773111621\n",
            "Time: 122.80083394050598 seconds\n",
            "Time cumulative: 7090.535531044006 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
            "FINE-TUNING\n",
            "Epoch: 30 test rmse: 0.8231593 test mae: 0.64167696 test ndcg: 0.9287297571033284\n",
            "Epoch: 30 train rmse: 0.6927848 train mae: 0.54303217 train ndcg: 0.9666979647994184\n",
            "Time: 122.53999996185303 seconds\n",
            "Time cumulative: 7213.075531005859 seconds\n",
            ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTi_PdXJqTjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84ba8de0-fc65-4442-fda3-697ed935f031"
      },
      "source": [
        "# Final result\n",
        "print('Epoch:', best_rmse_ep, ' best rmse:', best_rmse)\n",
        "print('Epoch:', best_mae_ep, ' best mae:', best_mae)\n",
        "print('Epoch:', best_ndcg_ep, ' best ndcg:', best_ndcg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23  best rmse: 0.8229355\n",
            "Epoch: 16  best mae: 0.6414332\n",
            "Epoch: 30  best ndcg: 0.9287297571033284\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat"
      ],
      "metadata": {
        "id": "Qf9cvmAvxGx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8877c76-7227-4dd7-8bd5-3cf1d158fa09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Sigmoid_5:0' shape=(3706, 500) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB0gtBXBl3YR",
        "outputId": "4835cee3-fa34-4e2e-b3e2-645d143c2986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Sigmoid_3:0' shape=(3706, 500) dtype=float32>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre"
      ],
      "metadata": {
        "id": "4KtaAIWRBZgL",
        "outputId": "0b203bb7-d7f0-4746-e714-9a8353fd345c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.3905616, 4.1434712, 3.8142185, ..., 3.5917108, 4.131508 ,\n",
              "        3.3984509],\n",
              "       [3.564693 , 3.1887875, 3.4957132, ..., 3.2197998, 3.205181 ,\n",
              "        1.59412  ],\n",
              "       [3.8023353, 3.278904 , 3.3012938, ..., 2.6215024, 2.8641865,\n",
              "        1.6721148],\n",
              "       ...,\n",
              "       [4.041753 , 3.6992843, 3.0517056, ..., 3.5361307, 3.803043 ,\n",
              "        4.0787   ],\n",
              "       [4.0346093, 3.49606  , 2.9894578, ..., 3.5343444, 3.6898644,\n",
              "        4.566532 ],\n",
              "       [3.917326 , 3.7951741, 3.2021036, ..., 4.0032816, 3.9122841,\n",
              "        4.233921 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre.shape #movie, user"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5TKq5dOmDis",
        "outputId": "03452d29-7e78-4b7d-98f4-614e280b560d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3706, 6040)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(np.transpose(pre))"
      ],
      "metadata": {
        "id": "2nB_LydZRT1a",
        "outputId": "5b49b283-6720-46d1-a386-9388bc15f8a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6040"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('movie_1m.dat','w')\n",
        "for i in range (len(np.transpose(pre))):\n",
        "  for j in range (len(pre)):\n",
        "    f.write('{}::{}::{} \\n '.format(i+1,j+1,round(np.transpose(pre)[i][j])))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "XcmTU-rK_MZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install neural-tangents\n",
        "!pip install -r /content/drive/MyDrive/infinite_ac_cf_main/requirements.txt\n",
        "!pip install sciPy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfVeDkMDlkqe",
        "outputId": "ed8a731d-281e-443d-e9f4-888413525054"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting neural-tangents\n",
            "  Downloading neural_tangents-0.6.1-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[K     |████████████████████████████████| 249 kB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jax>=0.3.13 in /usr/local/lib/python3.7/dist-packages (from neural-tangents) (0.3.23)\n",
            "Collecting tf2jax>=0.3.0\n",
            "  Downloading tf2jax-0.3.1-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from neural-tangents) (4.1.1)\n",
            "Collecting frozendict>=2.3\n",
            "  Downloading frozendict-2.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->neural-tangents) (1.3.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->neural-tangents) (0.9.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->neural-tangents) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->neural-tangents) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.13->neural-tangents) (3.3.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from tf2jax>=0.3.0->neural-tangents) (0.3.22+cuda11.cudnn805)\n",
            "Requirement already satisfied: tensorflow>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tf2jax>=0.3.0->neural-tangents) (2.9.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from tf2jax>=0.3.0->neural-tangents) (0.1.7)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (2.9.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (14.0.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (0.27.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.12)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.1.2)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (2.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (21.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.50.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (2.14.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (3.2.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.13->neural-tangents) (5.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow>=2.8.0->tf2jax>=0.3.0->neural-tangents) (3.0.9)\n",
            "Installing collected packages: tf2jax, frozendict, neural-tangents\n",
            "Successfully installed frozendict-2.3.4 neural-tangents-0.6.1 tf2jax-0.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jax>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/infinite_ac_cf_main/requirements.txt (line 1)) (0.3.23)\n",
            "Requirement already satisfied: h5py>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/infinite_ac_cf_main/requirements.txt (line 2)) (3.1.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement scipy>=1.8.0 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.5.0rc1, 1.5.0rc2, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0rc1, 1.6.0rc2, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.7.0rc1, 1.7.0rc2, 1.7.0, 1.7.1, 1.7.2, 1.7.3)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for scipy>=1.8.0\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sciPy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from sciPy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5dK8cHMnN4T",
        "outputId": "a2789eaa-a10f-4407-b258-a3089a5128ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.4135394, 4.12291  , 3.2787788, ..., 4.3908305, 4.373799 ,\n",
              "        3.6966176],\n",
              "       [2.9125788, 2.9991517, 2.571378 , ..., 3.3772337, 3.6530075,\n",
              "        3.5007908],\n",
              "       [3.2719262, 2.693303 , 2.747705 , ..., 3.2841537, 2.8917696,\n",
              "        3.718302 ],\n",
              "       ...,\n",
              "       [2.9467444, 3.2135677, 2.90559  , ..., 3.6273792, 3.8319266,\n",
              "        2.0522108],\n",
              "       [2.4696963, 2.8534532, 2.5415885, ..., 3.312544 , 3.673776 ,\n",
              "        2.3020263],\n",
              "       [2.7655137, 2.756269 , 2.8474503, ..., 3.2122643, 3.0982168,\n",
              "        2.6500058]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('GlocalK_weight_m1',pre)"
      ],
      "metadata": {
        "id": "_5kOiEGCot6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/drive/MyDrive/GlocalK_weight_m1', pre)"
      ],
      "metadata": {
        "id": "rCwvxnpQpXBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p data/\n",
        "!wget https://files.grouplens.org/datasets/movielens/ml-1m.zip -P data/\n",
        "!cd data/ ; unzip ml-1m.zip ; rm ml-1m.zip ; cd .."
      ],
      "metadata": {
        "id": "Df0InOCOa7yE",
        "outputId": "5bf0df27-eddc-4322-9d65-546741f6dd59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-13 09:57:07--  https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5917549 (5.6M) [application/zip]\n",
            "Saving to: ‘data/ml-1m.zip’\n",
            "\n",
            "ml-1m.zip           100%[===================>]   5.64M  12.4MB/s    in 0.5s    \n",
            "\n",
            "2022-11-13 09:57:08 (12.4 MB/s) - ‘data/ml-1m.zip’ saved [5917549/5917549]\n",
            "\n",
            "Archive:  ml-1m.zip\n",
            "   creating: ml-1m/\n",
            "  inflating: ml-1m/movies.dat        \n",
            "  inflating: ml-1m/ratings.dat       \n",
            "  inflating: ml-1m/README            \n",
            "  inflating: ml-1m/users.dat         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph processing"
      ],
      "metadata": {
        "id": "Sjg_DBOGRoic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!mkdir -p data/\n",
        "#!wget https://files.grouplens.org/datasets/movielens/ml-1m.zip -P data/\n",
        "#!cd data/ ; unzip ml-1m.zip ; rm ml-1m.zip ; cd ..\n",
        "!python preprocess.py movie_1m.dat"
      ],
      "metadata": {
        "id": "-gtLqK8_NW-v",
        "outputId": "bad6cd89-7cbe-45ce-e921-a1bfc8b10a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "!!!!!!!! STARTED PROCESSING movie_1m.dat !!!!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just infinite ae"
      ],
      "metadata": {
        "id": "8wxIDuT7RpOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python main.py"
      ],
      "metadata": {
        "id": "YHPgco6eNmmz",
        "outputId": "8dbbadfe-f1cc-4e2c-a408-d42fd0a0bfdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.000e+00 1.000e+00 4.000e+00]\n",
            " [0.000e+00 2.000e+00 4.000e+00]\n",
            " [0.000e+00 3.000e+00 4.000e+00]\n",
            " ...\n",
            " [6.039e+03 3.704e+03 4.000e+00]\n",
            " [6.039e+03 3.705e+03 5.000e+00]\n",
            " [6.039e+03 3.706e+03 4.000e+00]]\n",
            "[0 0 0 ... 0 0 0]\n",
            "# users: 6040\n",
            "# items: 3706\n",
            "# interactions: 17896520\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 79, in <module>\n",
            "    main(hyper_params)\n",
            "  File \"main.py\", line 75, in main\n",
            "    return train(hyper_params, data)\n",
            "  File \"main.py\", line 14, in train\n",
            "    from model import make_kernelized_rr_forward\n",
            "ImportError: cannot import name 'make_kernelized_rr_forward' from 'model' (/content/model.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
        "os.environ[\"TF_FORCE_UNIFIED_MEMORY\"] = \"1\"\n",
        "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from utils import log_end_epoch, get_item_propensity, get_common_path\n",
        "\n",
        "def train(hyper_params, data):\n",
        "    from model import make_kernelized_rr_forward\n",
        "    from eval import evaluate\n",
        "\n",
        "    # This just instantiates the function\n",
        "    kernelized_rr_forward, kernel_fn = make_kernelized_rr_forward(hyper_params)\n",
        "    sampled_matrix = data.sample_users(hyper_params['user_support']) # Random user sample\n",
        "\n",
        "    '''\n",
        "    NOTE: No training required! We will compute dual-variables \\alpha on the fly in `kernelized_rr_forward`\n",
        "          However, if we needed to perform evaluation multiple times, we could pre-compute \\alpha like so:\n",
        "    \n",
        "    import jax, jax.numpy as jnp, jax.scipy as sp\n",
        "    @jax.jit\n",
        "    def precompute_alpha(X, lamda=0.1):\n",
        "        K = kernel_fn(X, X)\n",
        "        K_reg = (K + jnp.abs(lamda) * jnp.trace(K) * jnp.eye(K.shape[0]) / K.shape[0])\n",
        "        return sp.linalg.solve(K_reg, X, sym_pos=True)\n",
        "    alpha = precompute_alpha(sampled_matrix, lamda=0.1) # Change for the desired value of lamda\n",
        "    '''\n",
        "\n",
        "    # Used for computing the PSP-metric\n",
        "    item_propensity = get_item_propensity(hyper_params, data)\n",
        "    \n",
        "    # Evaluation\n",
        "    start_time = time.time()\n",
        "\n",
        "    VAL_METRIC = \"HR@100\"\n",
        "    best_metric, best_lamda = None, None\n",
        "\n",
        "    # Validate on the validation-set\n",
        "    for lamda in [ 0.0, 1.0, 5.0, 20.0, 50.0, 100.0 ] if hyper_params['grid_search_lamda'] else [ hyper_params['lamda'] ]:\n",
        "        hyper_params['lamda'] = lamda\n",
        "        val_metrics = evaluate(hyper_params, kernelized_rr_forward, data, item_propensity, sampled_matrix)\n",
        "        if (best_metric is None) or (val_metrics[VAL_METRIC] > best_metric): best_metric, best_lamda = val_metrics[VAL_METRIC], lamda\n",
        "\n",
        "    # Return metrics with the best lamda on the test-set\n",
        "    hyper_params['lamda'] = best_lamda\n",
        "    test_metrics = evaluate(hyper_params, kernelized_rr_forward, data, item_propensity, sampled_matrix, test_set_eval = True)\n",
        "    \n",
        "    log_end_epoch(hyper_params, test_metrics, 0, time.time() - start_time)\n",
        "    start_time = time.time()\n",
        "\n",
        "    return test_metrics\n",
        "\n",
        "def main(hyper_params, gpu_id = None):\n",
        "    if gpu_id is not None: os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "\n",
        "    from jax.config import config\n",
        "    if 'float64' in hyper_params and hyper_params['float64'] == True: config.update('jax_enable_x64', True)\n",
        "\n",
        "    from data import Dataset\n",
        "\n",
        "    np.random.seed(hyper_params['seed'])\n",
        "    random.seed(hyper_params['seed'])\n",
        "\n",
        "    os.makedirs(\"./results/logs/\", exist_ok=True)\n",
        "    hyper_params['log_file'] = \"./results/logs/\" + get_common_path(hyper_params) + \".txt\"\n",
        "    \n",
        "    data = Dataset(hyper_params)\n",
        "    hyper_params = copy.deepcopy(data.hyper_params) # Updated w/ data-stats\n",
        "\n",
        "    return train(hyper_params, data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from hyper_params import hyper_params\n",
        "    main(hyper_params)"
      ],
      "metadata": {
        "id": "8JUzqwKOoV3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py, sys, os\n",
        "\n",
        "BASE_PATH = \"/\"\n",
        "#base_path\n",
        "\n",
        "def prep_movielens(ratings_file_path):\n",
        "    f = open(ratings_file_path, \"r\")\n",
        "    users, items, ratings = [], [], []\n",
        "\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        u, i, r = line.strip().split(\"::\")\n",
        "        users.append(int(u))\n",
        "        items.append(int(i))\n",
        "        ratings.append(float(r))\n",
        "        line = f.readline()\n",
        "\n",
        "    min_user = min(users)\n",
        "    num_users = len(set(users))\n",
        "\n",
        "    data = [ [] for _ in range(num_users) ]\n",
        "    for i in range(len(users)):\n",
        "        data[users[i] - min_user].append([ items[i], ratings[i] ])"
      ],
      "metadata": {
        "id": "2RK4iaFTWGuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distill Cf + Infinite ae"
      ],
      "metadata": {
        "id": "ObOtkbtWJdgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python grid_search_distill.py"
      ],
      "metadata": {
        "id": "6uvwWmG_JcXW",
        "outputId": "d41e4953-1d5c-4234-dce3-df39174fe2c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total processes before unique: 264\n",
            "Total processes after unique: 264\n",
            "Total processes after removing already finished jobs: 264\n",
            "{'ml-1m'}\n",
            "Loading ml-1m\n",
            "[[0.000e+00 1.000e+00 4.000e+00]\n",
            " [0.000e+00 2.000e+00 4.000e+00]\n",
            " [0.000e+00 3.000e+00 4.000e+00]\n",
            " ...\n",
            " [6.039e+03 3.704e+03 4.000e+00]\n",
            " [6.039e+03 3.705e+03 5.000e+00]\n",
            " [6.039e+03 3.706e+03 4.000e+00]]\n",
            "[0 0 0 ... 0 0 0]\n",
            "# users: 6040\n",
            "# items: 3706\n",
            "# interactions: 17896520\n",
            "| end of step    0 | time = 64.73 | HR@10 = 49.8460 | HR@100 = 50.0240 | NDCG@10 = 49.7667 | NDCG@100 = 49.9679 | PSP@10 = 4.9765 | PSP@100 = 0.4998 | AUC = 0.5006 | num_users = 10.0000 (VAL)\n",
            "| end of step   25 | time = 53.17 | HR@10 = 49.7351 | HR@100 = 49.8828 | NDCG@10 = 49.7842 | NDCG@100 = 49.8727 | PSP@10 = 4.9658 | PSP@100 = 0.4984 | AUC = 0.5006 | num_users = 10.0000 (VAL)\n",
            "| end of step   40 | time = 42.93 | HR@10 = 49.9056 | HR@100 = 50.0311 | NDCG@10 = 49.9854 | NDCG@100 = 50.0196 | PSP@10 = 4.9828 | PSP@100 = 0.4999 | AUC = 0.5004 | num_users = 10.0000 (VAL)\n",
            "| end of step   50 | time = 42.43 | HR@10 = 49.9702 | HR@100 = 49.8959 | NDCG@10 = 50.0840 | NDCG@100 = 49.9275 | PSP@10 = 4.9900 | PSP@100 = 0.4985 | AUC = 0.4997 | num_users = 10.0000 (VAL)\n",
            "| end of step   75 | time = 46.92 | HR@10 = 50.0695 | HR@100 = 49.9980 | NDCG@10 = 49.9601 | NDCG@100 = 49.9992 | PSP@10 = 4.9989 | PSP@100 = 0.4995 | AUC = 0.5006 | num_users = 10.0000 (VAL)\n",
            "| end of step   80 | time = 40.21 | HR@10 = 50.2103 | HR@100 = 49.9305 | NDCG@10 = 50.1996 | NDCG@100 = 49.9611 | PSP@10 = 5.0128 | PSP@100 = 0.4989 | AUC = 0.4994 | num_users = 10.0000 (VAL)\n",
            "| end of step  100 | time = 44.67 | HR@10 = 49.9023 | HR@100 = 49.9406 | NDCG@10 = 49.9907 | NDCG@100 = 49.9465 | PSP@10 = 4.9819 | PSP@100 = 0.4990 | AUC = 0.5007 | num_users = 10.0000 (VAL)\n",
            "| end of step  120 | time = 45.01 | HR@10 = 49.8825 | HR@100 = 49.8614 | NDCG@10 = 49.8658 | NDCG@100 = 49.8743 | PSP@10 = 4.9796 | PSP@100 = 0.4982 | AUC = 0.4993 | num_users = 10.0000 (VAL)\n",
            "| end of step  125 | time = 39.90 | HR@10 = 49.9205 | HR@100 = 49.9227 | NDCG@10 = 50.0553 | NDCG@100 = 49.9507 | PSP@10 = 4.9841 | PSP@100 = 0.4988 | AUC = 0.4999 | num_users = 10.0000 (VAL)\n",
            "| end of step  150 | time = 45.45 | HR@10 = 49.8576 | HR@100 = 49.9805 | NDCG@10 = 49.8199 | NDCG@100 = 49.9395 | PSP@10 = 4.9781 | PSP@100 = 0.4994 | AUC = 0.4992 | num_users = 10.0000 (VAL)\n",
            "| end of step  160 | time = 42.63 | HR@10 = 49.6954 | HR@100 = 49.8853 | NDCG@10 = 49.6930 | NDCG@100 = 49.8614 | PSP@10 = 4.9623 | PSP@100 = 0.4984 | AUC = 0.4990 | num_users = 10.0000 (VAL)\n",
            "| end of step  175 | time = 42.11 | HR@10 = 49.8957 | HR@100 = 50.0434 | NDCG@10 = 49.9812 | NDCG@100 = 50.0311 | PSP@10 = 4.9812 | PSP@100 = 0.5000 | AUC = 0.4998 | num_users = 10.0000 (VAL)\n",
            "| end of step  200 | time = 45.97 | HR@10 = 49.9586 | HR@100 = 49.9056 | NDCG@10 = 49.8244 | NDCG@100 = 49.8917 | PSP@10 = 4.9878 | PSP@100 = 0.4986 | AUC = 0.4989 | num_users = 10.0000 (VAL)\n",
            "| end of step  240 | time = 49.08 | HR@10 = 49.9073 | HR@100 = 49.9975 | NDCG@10 = 49.9267 | NDCG@100 = 49.9979 | PSP@10 = 4.9830 | PSP@100 = 0.4995 | AUC = 0.5003 | num_users = 10.0000 (VAL)\n",
            "| end of step  280 | time = 47.14 | HR@10 = 49.9354 | HR@100 = 49.9692 | NDCG@10 = 49.9339 | NDCG@100 = 49.9579 | PSP@10 = 4.9850 | PSP@100 = 0.4992 | AUC = 0.5005 | num_users = 10.0000 (VAL)\n",
            "| end of step  320 | time = 47.12 | HR@10 = 50.2417 | HR@100 = 49.9022 | NDCG@10 = 50.5060 | NDCG@100 = 50.0131 | PSP@10 = 5.0157 | PSP@100 = 0.4986 | AUC = 0.4997 | num_users = 10.0000 (VAL)\n",
            "| end of step  360 | time = 48.36 | HR@10 = 50.0795 | HR@100 = 49.9215 | NDCG@10 = 50.0887 | NDCG@100 = 49.9355 | PSP@10 = 4.9999 | PSP@100 = 0.4988 | AUC = 0.4997 | num_users = 10.0000 (VAL)\n",
            "Exiting early...\n",
            "| end of step  175 | time = 834.95 | HR@10 = 100.0000 | HR@100 = 100.0000 | NDCG@10 = 100.0000 | NDCG@100 = 100.0000 | PSP@10 = 9.9837 | PSP@100 = 0.9991 | AUC = 0.4986 | num_users = 10.0000 (TEST)\n",
            "| end of step    0 | time = 56.24 | HR@10 = 49.9040 | HR@100 = 49.9912 | NDCG@10 = 49.9721 | NDCG@100 = 49.9804 | PSP@10 = 4.9822 | PSP@100 = 0.4995 | AUC = 0.5004 | num_users = 20.0000 (VAL)\n",
            "| end of step   25 | time = 49.38 | HR@10 = 49.6887 | HR@100 = 49.9055 | NDCG@10 = 49.7845 | NDCG@100 = 49.8828 | PSP@10 = 4.9592 | PSP@100 = 0.4985 | AUC = 0.5000 | num_users = 20.0000 (VAL)\n",
            "| end of step   40 | time = 43.53 | HR@10 = 50.2219 | HR@100 = 49.9265 | NDCG@10 = 50.2016 | NDCG@100 = 49.9563 | PSP@10 = 5.0137 | PSP@100 = 0.4988 | AUC = 0.4998 | num_users = 20.0000 (VAL)\n",
            "| end of step   50 | time = 43.52 | HR@10 = 50.1573 | HR@100 = 49.9680 | NDCG@10 = 50.1515 | NDCG@100 = 49.9948 | PSP@10 = 5.0074 | PSP@100 = 0.4992 | AUC = 0.5002 | num_users = 20.0000 (VAL)\n",
            "| end of step   75 | time = 44.54 | HR@10 = 50.0116 | HR@100 = 49.8891 | NDCG@10 = 50.0518 | NDCG@100 = 49.8973 | PSP@10 = 4.9925 | PSP@100 = 0.4984 | AUC = 0.4998 | num_users = 20.0000 (VAL)\n",
            "| end of step   80 | time = 44.25 | HR@10 = 49.7467 | HR@100 = 49.8975 | NDCG@10 = 49.7997 | NDCG@100 = 49.8951 | PSP@10 = 4.9646 | PSP@100 = 0.4985 | AUC = 0.4989 | num_users = 20.0000 (VAL)\n",
            "| end of step  100 | time = 43.46 | HR@10 = 49.8742 | HR@100 = 49.9796 | NDCG@10 = 49.8924 | NDCG@100 = 49.9617 | PSP@10 = 4.9794 | PSP@100 = 0.4993 | AUC = 0.4993 | num_users = 20.0000 (VAL)\n",
            "| end of step  120 | time = 44.64 | HR@10 = 49.7053 | HR@100 = 49.9005 | NDCG@10 = 49.7092 | NDCG@100 = 49.8819 | PSP@10 = 4.9623 | PSP@100 = 0.4985 | AUC = 0.5006 | num_users = 20.0000 (VAL)\n",
            "| end of step  125 | time = 42.01 | HR@10 = 50.1474 | HR@100 = 49.9184 | NDCG@10 = 50.2917 | NDCG@100 = 49.9653 | PSP@10 = 5.0066 | PSP@100 = 0.4987 | AUC = 0.5001 | num_users = 20.0000 (VAL)\n",
            "| end of step  150 | time = 45.06 | HR@10 = 49.9669 | HR@100 = 50.0134 | NDCG@10 = 49.8531 | NDCG@100 = 49.9748 | PSP@10 = 4.9876 | PSP@100 = 0.4997 | AUC = 0.4998 | num_users = 20.0000 (VAL)\n",
            "| end of step  160 | time = 42.44 | HR@10 = 49.8626 | HR@100 = 49.9503 | NDCG@10 = 49.9495 | NDCG@100 = 49.9564 | PSP@10 = 4.9779 | PSP@100 = 0.4991 | AUC = 0.5006 | num_users = 20.0000 (VAL)\n",
            "| end of step  175 | time = 44.18 | HR@10 = 49.8344 | HR@100 = 49.8965 | NDCG@10 = 49.7966 | NDCG@100 = 49.8784 | PSP@10 = 4.9752 | PSP@100 = 0.4985 | AUC = 0.4991 | num_users = 20.0000 (VAL)\n",
            "| end of step  200 | time = 45.46 | HR@10 = 50.0546 | HR@100 = 49.9546 | NDCG@10 = 50.0814 | NDCG@100 = 49.9721 | PSP@10 = 4.9974 | PSP@100 = 0.4991 | AUC = 0.5004 | num_users = 20.0000 (VAL)\n",
            "| end of step  240 | time = 48.22 | HR@10 = 49.8808 | HR@100 = 49.9427 | NDCG@10 = 49.9427 | NDCG@100 = 49.9454 | PSP@10 = 4.9805 | PSP@100 = 0.4990 | AUC = 0.5002 | num_users = 20.0000 (VAL)\n",
            "| end of step  280 | time = 49.80 | HR@10 = 49.8907 | HR@100 = 50.0359 | NDCG@10 = 49.9242 | NDCG@100 = 50.0155 | PSP@10 = 4.9809 | PSP@100 = 0.4999 | AUC = 0.4996 | num_users = 20.0000 (VAL)\n",
            "| end of step  320 | time = 49.09 | HR@10 = 49.6606 | HR@100 = 50.0291 | NDCG@10 = 49.7468 | NDCG@100 = 50.0015 | PSP@10 = 4.9580 | PSP@100 = 0.4998 | AUC = 0.5004 | num_users = 20.0000 (VAL)\n",
            "| end of step  360 | time = 48.23 | HR@10 = 49.9586 | HR@100 = 49.9555 | NDCG@10 = 50.1050 | NDCG@100 = 49.9794 | PSP@10 = 4.9882 | PSP@100 = 0.4991 | AUC = 0.5004 | num_users = 20.0000 (VAL)\n",
            "| end of step  400 | time = 47.69 | HR@10 = 49.7781 | HR@100 = 49.9450 | NDCG@10 = 49.7080 | NDCG@100 = 49.8960 | PSP@10 = 4.9702 | PSP@100 = 0.4990 | AUC = 0.5003 | num_users = 20.0000 (VAL)\n",
            "| end of step  440 | time = 51.63 | HR@10 = 49.9056 | HR@100 = 49.9485 | NDCG@10 = 49.8994 | NDCG@100 = 49.9481 | PSP@10 = 4.9825 | PSP@100 = 0.4991 | AUC = 0.4995 | num_users = 20.0000 (VAL)\n",
            "Exiting early...\n",
            "| end of step  280 | time = 930.98 | HR@10 = 100.0000 | HR@100 = 100.0000 | NDCG@10 = 100.0000 | NDCG@100 = 100.0000 | PSP@10 = 9.9837 | PSP@100 = 0.9991 | AUC = 0.4983 | num_users = 20.0000 (TEST)\n",
            "| end of step    0 | time = 56.82 | HR@10 = 50.0298 | HR@100 = 49.8945 | NDCG@10 = 49.9192 | NDCG@100 = 49.8840 | PSP@10 = 4.9947 | PSP@100 = 0.4985 | AUC = 0.5001 | num_users = 40.0000 (VAL)\n",
            "| end of step   25 | time = 48.94 | HR@10 = 49.9669 | HR@100 = 49.8921 | NDCG@10 = 50.1214 | NDCG@100 = 49.9194 | PSP@10 = 4.9884 | PSP@100 = 0.4985 | AUC = 0.4999 | num_users = 40.0000 (VAL)\n",
            "| end of step   40 | time = 44.01 | HR@10 = 49.7781 | HR@100 = 49.9536 | NDCG@10 = 49.8954 | NDCG@100 = 49.9658 | PSP@10 = 4.9696 | PSP@100 = 0.4991 | AUC = 0.4998 | num_users = 40.0000 (VAL)\n",
            "| end of step   50 | time = 43.65 | HR@10 = 49.9056 | HR@100 = 49.9611 | NDCG@10 = 49.8837 | NDCG@100 = 49.9479 | PSP@10 = 4.9822 | PSP@100 = 0.4992 | AUC = 0.4996 | num_users = 40.0000 (VAL)\n",
            "| end of step   75 | time = 44.49 | HR@10 = 49.9123 | HR@100 = 49.9374 | NDCG@10 = 49.8608 | NDCG@100 = 49.9325 | PSP@10 = 4.9830 | PSP@100 = 0.4989 | AUC = 0.5004 | num_users = 40.0000 (VAL)\n",
            "| end of step   80 | time = 42.19 | HR@10 = 49.6921 | HR@100 = 49.9354 | NDCG@10 = 49.7443 | NDCG@100 = 49.8952 | PSP@10 = 4.9616 | PSP@100 = 0.4989 | AUC = 0.5000 | num_users = 40.0000 (VAL)\n",
            "| end of step  100 | time = 45.47 | HR@10 = 50.1788 | HR@100 = 50.0399 | NDCG@10 = 50.3447 | NDCG@100 = 50.0811 | PSP@10 = 5.0100 | PSP@100 = 0.5000 | AUC = 0.4993 | num_users = 40.0000 (VAL)\n",
            "| end of step  120 | time = 43.87 | HR@10 = 49.8808 | HR@100 = 49.9132 | NDCG@10 = 50.0804 | NDCG@100 = 49.9493 | PSP@10 = 4.9794 | PSP@100 = 0.4987 | AUC = 0.4998 | num_users = 40.0000 (VAL)\n",
            "| end of step  125 | time = 42.18 | HR@10 = 49.9586 | HR@100 = 49.9704 | NDCG@10 = 49.9128 | NDCG@100 = 49.9647 | PSP@10 = 4.9871 | PSP@100 = 0.4992 | AUC = 0.4998 | num_users = 40.0000 (VAL)\n",
            "| end of step  150 | time = 46.71 | HR@10 = 50.0364 | HR@100 = 49.8381 | NDCG@10 = 50.2496 | NDCG@100 = 49.9072 | PSP@10 = 4.9951 | PSP@100 = 0.4979 | AUC = 0.5002 | num_users = 40.0000 (VAL)\n",
            "| end of step  160 | time = 41.53 | HR@10 = 50.1209 | HR@100 = 49.9480 | NDCG@10 = 50.2150 | NDCG@100 = 50.0003 | PSP@10 = 5.0041 | PSP@100 = 0.4990 | AUC = 0.5004 | num_users = 40.0000 (VAL)\n",
            "| end of step  175 | time = 43.68 | HR@10 = 49.8874 | HR@100 = 50.0172 | NDCG@10 = 50.0322 | NDCG@100 = 50.0437 | PSP@10 = 4.9802 | PSP@100 = 0.4997 | AUC = 0.5006 | num_users = 40.0000 (VAL)\n",
            "| end of step  200 | time = 46.88 | HR@10 = 50.1573 | HR@100 = 49.9184 | NDCG@10 = 50.1988 | NDCG@100 = 49.9541 | PSP@10 = 5.0072 | PSP@100 = 0.4987 | AUC = 0.4998 | num_users = 40.0000 (VAL)\n",
            "| end of step  240 | time = 49.24 | HR@10 = 49.7864 | HR@100 = 49.8945 | NDCG@10 = 49.6505 | NDCG@100 = 49.8321 | PSP@10 = 4.9703 | PSP@100 = 0.4985 | AUC = 0.4994 | num_users = 40.0000 (VAL)\n",
            "| end of step  280 | time = 48.50 | HR@10 = 49.8295 | HR@100 = 49.9404 | NDCG@10 = 49.8102 | NDCG@100 = 49.9146 | PSP@10 = 4.9748 | PSP@100 = 0.4990 | AUC = 0.5002 | num_users = 40.0000 (VAL)\n",
            "Exiting early...\n",
            "| end of step  100 | time = 735.56 | HR@10 = 100.0000 | HR@100 = 100.0000 | NDCG@10 = 100.0000 | NDCG@100 = 100.0000 | PSP@10 = 9.9846 | PSP@100 = 0.9991 | AUC = 0.4929 | num_users = 40.0000 (TEST)\n",
            "| end of step    0 | time = 57.70 | HR@10 = 50.0662 | HR@100 = 49.8844 | NDCG@10 = 50.1611 | NDCG@100 = 49.9416 | PSP@10 = 4.9983 | PSP@100 = 0.4984 | AUC = 0.5004 | num_users = 80.0000 (VAL)\n",
            "| end of step   25 | time = 49.88 | HR@10 = 50.1142 | HR@100 = 49.8871 | NDCG@10 = 50.0013 | NDCG@100 = 49.8885 | PSP@10 = 5.0031 | PSP@100 = 0.4984 | AUC = 0.4991 | num_users = 80.0000 (VAL)\n",
            "| end of step   40 | time = 42.52 | HR@10 = 49.8046 | HR@100 = 49.9013 | NDCG@10 = 49.8551 | NDCG@100 = 49.8848 | PSP@10 = 4.9722 | PSP@100 = 0.4986 | AUC = 0.4993 | num_users = 80.0000 (VAL)\n",
            "| end of step   50 | time = 43.51 | HR@10 = 50.2781 | HR@100 = 49.9195 | NDCG@10 = 50.1625 | NDCG@100 = 49.9275 | PSP@10 = 5.0192 | PSP@100 = 0.4987 | AUC = 0.4993 | num_users = 80.0000 (VAL)\n",
            "| end of step   75 | time = 46.25 | HR@10 = 50.2202 | HR@100 = 49.9755 | NDCG@10 = 50.2587 | NDCG@100 = 50.0094 | PSP@10 = 5.0133 | PSP@100 = 0.4993 | AUC = 0.4994 | num_users = 80.0000 (VAL)\n",
            "| end of step   80 | time = 41.03 | HR@10 = 50.3758 | HR@100 = 49.9478 | NDCG@10 = 50.4271 | NDCG@100 = 50.0206 | PSP@10 = 5.0281 | PSP@100 = 0.4990 | AUC = 0.5002 | num_users = 80.0000 (VAL)\n",
            "| end of step  100 | time = 44.86 | HR@10 = 49.6258 | HR@100 = 49.9624 | NDCG@10 = 49.7665 | NDCG@100 = 49.9517 | PSP@10 = 4.9539 | PSP@100 = 0.4991 | AUC = 0.5005 | num_users = 80.0000 (VAL)\n",
            "| end of step  120 | time = 45.80 | HR@10 = 50.2351 | HR@100 = 49.9682 | NDCG@10 = 50.4247 | NDCG@100 = 50.0407 | PSP@10 = 5.0151 | PSP@100 = 0.4992 | AUC = 0.4997 | num_users = 80.0000 (VAL)\n",
            "| end of step  125 | time = 40.26 | HR@10 = 49.8460 | HR@100 = 49.9371 | NDCG@10 = 49.8930 | NDCG@100 = 49.9231 | PSP@10 = 4.9761 | PSP@100 = 0.4989 | AUC = 0.5000 | num_users = 80.0000 (VAL)\n",
            "| end of step  150 | time = 45.72 | HR@10 = 50.0579 | HR@100 = 50.0589 | NDCG@10 = 50.0214 | NDCG@100 = 50.0606 | PSP@10 = 4.9966 | PSP@100 = 0.5001 | AUC = 0.5002 | num_users = 80.0000 (VAL)\n",
            "| end of step  160 | time = 42.77 | HR@10 = 49.7169 | HR@100 = 49.9320 | NDCG@10 = 49.7344 | NDCG@100 = 49.9026 | PSP@10 = 4.9635 | PSP@100 = 0.4989 | AUC = 0.4990 | num_users = 80.0000 (VAL)\n",
            "| end of step  175 | time = 42.35 | HR@10 = 49.9040 | HR@100 = 49.9280 | NDCG@10 = 49.6561 | NDCG@100 = 49.8741 | PSP@10 = 4.9820 | PSP@100 = 0.4988 | AUC = 0.4996 | num_users = 80.0000 (VAL)\n",
            "| end of step  200 | time = 46.70 | HR@10 = 49.7103 | HR@100 = 49.9823 | NDCG@10 = 49.6888 | NDCG@100 = 49.9432 | PSP@10 = 4.9630 | PSP@100 = 0.4994 | AUC = 0.4999 | num_users = 80.0000 (VAL)\n",
            "| end of step  240 | time = 49.24 | HR@10 = 50.1325 | HR@100 = 50.0101 | NDCG@10 = 50.0641 | NDCG@100 = 50.0030 | PSP@10 = 5.0047 | PSP@100 = 0.4996 | AUC = 0.5003 | num_users = 80.0000 (VAL)\n",
            "| end of step  280 | time = 50.40 | HR@10 = 50.2815 | HR@100 = 49.9715 | NDCG@10 = 50.2718 | NDCG@100 = 50.0078 | PSP@10 = 5.0198 | PSP@100 = 0.4992 | AUC = 0.5000 | num_users = 80.0000 (VAL)\n",
            "| end of step  320 | time = 48.10 | HR@10 = 50.0232 | HR@100 = 49.9775 | NDCG@10 = 50.0722 | NDCG@100 = 49.9791 | PSP@10 = 4.9937 | PSP@100 = 0.4993 | AUC = 0.5001 | num_users = 80.0000 (VAL)\n",
            "Exiting early...\n",
            "| end of step  150 | time = 785.28 | HR@10 = 100.0000 | HR@100 = 100.0000 | NDCG@10 = 100.0000 | NDCG@100 = 100.0000 | PSP@10 = 9.9817 | PSP@100 = 0.9991 | AUC = 0.4810 | num_users = 80.0000 (TEST)\n",
            "| end of step    0 | time = 57.62 | HR@10 = 49.7235 | HR@100 = 49.8609 | NDCG@10 = 49.7053 | NDCG@100 = 49.8339 | PSP@10 = 4.9632 | PSP@100 = 0.4981 | AUC = 0.4999 | num_users = 100.0000 (VAL)\n",
            "| end of step   25 | time = 49.71 | HR@10 = 50.2897 | HR@100 = 49.9439 | NDCG@10 = 50.3233 | NDCG@100 = 50.0014 | PSP@10 = 5.0204 | PSP@100 = 0.4990 | AUC = 0.4996 | num_users = 100.0000 (VAL)\n",
            "| end of step   40 | time = 42.49 | HR@10 = 49.9652 | HR@100 = 50.0180 | NDCG@10 = 49.9436 | NDCG@100 = 50.0033 | PSP@10 = 4.9880 | PSP@100 = 0.4997 | AUC = 0.5005 | num_users = 100.0000 (VAL)\n",
            "| end of step   50 | time = 42.63 | HR@10 = 49.9106 | HR@100 = 49.9977 | NDCG@10 = 50.0884 | NDCG@100 = 50.0166 | PSP@10 = 4.9826 | PSP@100 = 0.4995 | AUC = 0.5001 | num_users = 100.0000 (VAL)\n",
            "| end of step   75 | time = 46.15 | HR@10 = 50.1523 | HR@100 = 50.0452 | NDCG@10 = 50.2010 | NDCG@100 = 50.0728 | PSP@10 = 5.0066 | PSP@100 = 0.5000 | AUC = 0.5003 | num_users = 100.0000 (VAL)\n",
            "| end of step   80 | time = 42.03 | HR@10 = 50.1192 | HR@100 = 49.8738 | NDCG@10 = 50.1976 | NDCG@100 = 49.9169 | PSP@10 = 5.0036 | PSP@100 = 0.4983 | AUC = 0.4999 | num_users = 100.0000 (VAL)\n",
            "| end of step  100 | time = 43.42 | HR@10 = 49.8841 | HR@100 = 49.9869 | NDCG@10 = 50.0963 | NDCG@100 = 50.0300 | PSP@10 = 4.9795 | PSP@100 = 0.4994 | AUC = 0.5002 | num_users = 100.0000 (VAL)\n",
            "| end of step  120 | time = 45.18 | HR@10 = 50.0695 | HR@100 = 49.9737 | NDCG@10 = 50.1516 | NDCG@100 = 50.0044 | PSP@10 = 4.9985 | PSP@100 = 0.4993 | AUC = 0.4998 | num_users = 100.0000 (VAL)\n",
            "| end of step  125 | time = 40.94 | HR@10 = 49.8940 | HR@100 = 49.8588 | NDCG@10 = 50.1070 | NDCG@100 = 49.9087 | PSP@10 = 4.9807 | PSP@100 = 0.4981 | AUC = 0.4997 | num_users = 100.0000 (VAL)\n",
            "| end of step  150 | time = 44.51 | HR@10 = 49.8493 | HR@100 = 49.9745 | NDCG@10 = 49.9496 | NDCG@100 = 49.9842 | PSP@10 = 4.9766 | PSP@100 = 0.4993 | AUC = 0.4994 | num_users = 100.0000 (VAL)\n",
            "| end of step  160 | time = 43.15 | HR@10 = 49.8725 | HR@100 = 50.0038 | NDCG@10 = 49.7848 | NDCG@100 = 49.9651 | PSP@10 = 4.9785 | PSP@100 = 0.4996 | AUC = 0.4998 | num_users = 100.0000 (VAL)\n",
            "| end of step  175 | time = 42.76 | HR@10 = 49.9056 | HR@100 = 49.9836 | NDCG@10 = 50.1682 | NDCG@100 = 50.0327 | PSP@10 = 4.9820 | PSP@100 = 0.4994 | AUC = 0.5001 | num_users = 100.0000 (VAL)\n",
            "| end of step  200 | time = 45.67 | HR@10 = 49.9586 | HR@100 = 49.8722 | NDCG@10 = 49.9652 | NDCG@100 = 49.8874 | PSP@10 = 4.9864 | PSP@100 = 0.4982 | AUC = 0.5000 | num_users = 100.0000 (VAL)\n",
            "| end of step  240 | time = 49.50 | HR@10 = 49.8758 | HR@100 = 49.8682 | NDCG@10 = 49.8374 | NDCG@100 = 49.8355 | PSP@10 = 4.9790 | PSP@100 = 0.4982 | AUC = 0.4996 | num_users = 100.0000 (VAL)\n",
            "Exiting early...\n",
            "| end of step   75 | time = 683.71 | HR@10 = 100.0000 | HR@100 = 100.0000 | NDCG@10 = 100.0000 | NDCG@100 = 100.0000 | PSP@10 = 9.9825 | PSP@100 = 0.9991 | AUC = 0.4913 | num_users = 100.0000 (TEST)\n",
            "| end of step    0 | time = 57.12 | HR@10 = 49.5877 | HR@100 = 49.9030 | NDCG@10 = 49.5381 | NDCG@100 = 49.8445 | PSP@10 = 4.9489 | PSP@100 = 0.4985 | AUC = 0.5001 | num_users = 197.0000 (VAL)\n",
            "| end of step   25 | time = 49.48 | HR@10 = 49.9454 | HR@100 = 49.9810 | NDCG@10 = 50.0982 | NDCG@100 = 49.9955 | PSP@10 = 4.9851 | PSP@100 = 0.4993 | AUC = 0.5005 | num_users = 197.0000 (VAL)\n",
            "| end of step   40 | time = 43.89 | HR@10 = 50.1358 | HR@100 = 49.9399 | NDCG@10 = 50.2240 | NDCG@100 = 49.9843 | PSP@10 = 5.0047 | PSP@100 = 0.4989 | AUC = 0.5004 | num_users = 197.0000 (VAL)\n",
            "| end of step   50 | time = 43.76 | HR@10 = 49.9404 | HR@100 = 50.0195 | NDCG@10 = 49.8499 | NDCG@100 = 49.9897 | PSP@10 = 4.9856 | PSP@100 = 0.4997 | AUC = 0.5009 | num_users = 197.0000 (VAL)\n",
            "| end of step   75 | time = 45.31 | HR@10 = 49.9106 | HR@100 = 49.9609 | NDCG@10 = 49.9951 | NDCG@100 = 49.9740 | PSP@10 = 4.9825 | PSP@100 = 0.4991 | AUC = 0.4998 | num_users = 197.0000 (VAL)\n",
            "| end of step   80 | time = 42.49 | HR@10 = 49.9669 | HR@100 = 49.8757 | NDCG@10 = 49.8147 | NDCG@100 = 49.8393 | PSP@10 = 4.9880 | PSP@100 = 0.4983 | AUC = 0.4999 | num_users = 197.0000 (VAL)\n",
            "| end of step  100 | time = 45.10 | HR@10 = 49.7136 | HR@100 = 49.9290 | NDCG@10 = 49.6477 | NDCG@100 = 49.9023 | PSP@10 = 4.9637 | PSP@100 = 0.4988 | AUC = 0.5005 | num_users = 197.0000 (VAL)\n",
            "| end of step  120 | time = 44.70 | HR@10 = 49.8361 | HR@100 = 49.9930 | NDCG@10 = 49.8623 | NDCG@100 = 49.9564 | PSP@10 = 4.9745 | PSP@100 = 0.4994 | AUC = 0.5003 | num_users = 197.0000 (VAL)\n",
            "| end of step  125 | time = 41.72 | HR@10 = 49.9868 | HR@100 = 49.9813 | NDCG@10 = 49.7627 | NDCG@100 = 49.9409 | PSP@10 = 4.9899 | PSP@100 = 0.4993 | AUC = 0.5008 | num_users = 197.0000 (VAL)\n",
            "| end of step  150 | time = 46.96 | HR@10 = 49.6043 | HR@100 = 49.9075 | NDCG@10 = 49.6008 | NDCG@100 = 49.8576 | PSP@10 = 4.9518 | PSP@100 = 0.4986 | AUC = 0.4992 | num_users = 197.0000 (VAL)\n",
            "| end of step  160 | time = 41.74 | HR@10 = 49.6589 | HR@100 = 49.9755 | NDCG@10 = 49.5525 | NDCG@100 = 49.9170 | PSP@10 = 4.9571 | PSP@100 = 0.4993 | AUC = 0.4999 | num_users = 197.0000 (VAL)\n",
            "Traceback (most recent call last):\n",
            "  File \"grid_search_distill.py\", line 149, in <module>\n",
            "    run_tasks(gpu_jobs[gpu], gpu_ids[gpu])\n",
            "  File \"grid_search_distill.py\", line 140, in run_tasks\n",
            "    try: main(task, data = data, gpu_id = gpu_id)\n",
            "  File \"/content/distill.py\", line 156, in main\n",
            "    params_final, test_metrics = train_complete(hyper_params, data)\n",
            "  File \"/content/distill.py\", line 112, in train_complete\n",
            "    metrics, best_lamda = evaluate_with_grid_search(hyper_params, kernelized_rr_forward, data, item_propensity, multi_gumbel_sampling(key, params['x'])[0])\n",
            "  File \"/content/eval.py\", line 13, in evaluate_with_grid_search\n",
            "    val_metrics = evaluate(hyper_params, kernelized_rr_forward, data, item_propensity, train_x, test_set_eval=False, topk=topk)\n",
            "  File \"/content/eval.py\", line 48, in evaluate\n",
            "    topk, metrics\n",
            "  File \"/content/eval.py\", line 79, in evaluate_batch\n",
            "    for b in range(len(logits)): logits[b][ train_positive[b] ] = -INF\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python distill.py"
      ],
      "metadata": {
        "id": "KZI73XU1Jcau",
        "outputId": "6004f840-7101-4110-9de8-b403342b6197",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.000e+00 1.000e+00 4.000e+00]\n",
            " [0.000e+00 2.000e+00 4.000e+00]\n",
            " [0.000e+00 3.000e+00 4.000e+00]\n",
            " ...\n",
            " [6.039e+03 3.704e+03 4.000e+00]\n",
            " [6.039e+03 3.705e+03 5.000e+00]\n",
            " [6.039e+03 3.706e+03 4.000e+00]]\n",
            "[0 0 0 ... 0 0 0]\n",
            "# users: 6040\n",
            "# items: 3706\n",
            "# interactions: 17896520\n",
            "Traceback (most recent call last):\n",
            "  File \"distill.py\", line 165, in <module>\n",
            "    main(hyper_params)\n",
            "  File \"distill.py\", line 156, in main\n",
            "    params_final, test_metrics = train_complete(hyper_params, data)\n",
            "  File \"distill.py\", line 84, in train_complete\n",
            "    opt_state, get_params, update_fn, kernelized_rr_forward, multi_gumbel_sampling, grad_zeros = get_update_functions(hyper_params, params_init)\n",
            "  File \"distill.py\", line 18, in get_update_functions\n",
            "    from model import make_loss_fn\n",
            "ImportError: cannot import name 'make_loss_fn' from 'model' (/content/model.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rnxzedM8VjOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}